<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RAILS â€” Evaluation Results</title>
<style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
    background: #f8f9fa;
    color: #1a1a2e;
    line-height: 1.6;
    padding: 2rem;
    max-width: 1100px;
    margin: 0 auto;
}
h1 { font-size: 1.8rem; margin-bottom: 0.3rem; color: #16213e; }
h2 { font-size: 1.3rem; color: #16213e; margin-bottom: 0.8rem; }
h3 { font-size: 1.05rem; color: #374151; margin-bottom: 0.5rem; }
.subtitle { color: #666; font-size: 0.95rem; margin-bottom: 2rem; }

/* Summary section */
.summary { background: white; border-radius: 12px; padding: 1.5rem; margin-bottom: 1.5rem; box-shadow: 0 1px 3px rgba(0,0,0,0.08); }
.stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(130px, 1fr)); gap: 0.8rem; margin-bottom: 1rem; }
.stat { background: #f0f4ff; border-radius: 8px; padding: 0.8rem; text-align: center; }
.stat-value { font-size: 1.5rem; font-weight: 700; color: #2563eb; }
.stat-label { font-size: 0.75rem; color: #666; margin-top: 0.2rem; }
.stat-green .stat-value { color: #16a34a; }
.stat-amber .stat-value { color: #d97706; }
.stat-purple .stat-value { color: #7c3aed; }

/* Cards */
.card { background: white; border-radius: 12px; padding: 1.5rem; margin-bottom: 1.5rem; box-shadow: 0 1px 3px rgba(0,0,0,0.08); }

/* Tables */
.table-wrapper { overflow-x: auto; margin: 1rem 0; }
table { width: 100%; border-collapse: collapse; font-size: 0.82rem; }
th { background: #f0f4ff; color: #1e40af; font-weight: 600; text-transform: uppercase; letter-spacing: 0.04em; font-size: 0.72rem; padding: 0.6rem 0.5rem; text-align: center; border-bottom: 2px solid #dbeafe; white-space: nowrap; }
th:first-child { text-align: left; }
td { padding: 0.5rem 0.5rem; text-align: center; border-bottom: 1px solid #f0f0f0; }
td:first-child { text-align: left; font-weight: 500; white-space: nowrap; }
tr:hover { background: #fafbff; }

/* Model group header rows */
.model-header td { background: #f8fafc; font-weight: 700; color: #16213e; border-bottom: 2px solid #e2e8f0; padding-top: 0.8rem; }
.model-header td:first-child { padding-left: 0; }

/* Condition indentation */
.condition-row td:first-child { padding-left: 1.2rem; font-weight: 400; color: #555; }

/* Highlight best value */
.best { font-weight: 700; color: #16a34a; }
.improved { background: #f0fdf4; }

/* Badges */
.badge { display: inline-block; padding: 0.15rem 0.5rem; border-radius: 20px; font-size: 0.72rem; font-weight: 600; }
.badge-blue { background: #e0e7ff; color: #3730a3; }
.badge-green { background: #dcfce7; color: #166534; }
.badge-amber { background: #fef3c7; color: #92400e; }
.badge-purple { background: #f3e8ff; color: #6b21a8; }

/* Delta indicators */
.delta-pos { color: #16a34a; font-size: 0.72rem; font-weight: 600; }
.delta-neg { color: #dc2626; font-size: 0.72rem; font-weight: 600; }

/* Findings */
.finding { padding: 0.8rem 1rem; border-radius: 8px; margin-bottom: 0.6rem; font-size: 0.88rem; }
.finding-positive { background: #f0fdf4; border-left: 4px solid #22c55e; }
.finding-insight { background: #eff6ff; border-left: 4px solid #3b82f6; }
.finding-caution { background: #fffbeb; border-left: 4px solid #f59e0b; }
.finding strong { color: #16213e; }

/* Section labels */
.section-label { font-size: 0.7rem; text-transform: uppercase; letter-spacing: 0.05em; color: #94a3b8; font-weight: 600; margin-bottom: 0.6rem; }

/* Legend */
.legend { display: flex; flex-wrap: wrap; gap: 0.6rem; margin: 0.8rem 0; font-size: 0.78rem; }
.legend-item { display: flex; align-items: center; gap: 0.3rem; }
.legend-dot { width: 10px; height: 10px; border-radius: 50%; }

/* Nav link */
.back-link { display: inline-block; margin-bottom: 1rem; color: #2563eb; text-decoration: none; font-size: 0.85rem; font-weight: 500; }
.back-link:hover { text-decoration: underline; }

/* Footer */
.footer { text-align: center; color: #94a3b8; font-size: 0.8rem; margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e5e7eb; }

/* Responsive */
@media (max-width: 768px) {
    body { padding: 1rem; }
    .stats { grid-template-columns: repeat(2, 1fr); }
}
</style>
</head>
<body>
<a href="index.html" class="back-link">&larr; Back to Hub</a>
<h1>RAILS &mdash; Evaluation Results</h1>
<p class="subtitle">Performance comparison &middot; DeepSeek-R1 &amp; Qwen3 &middot; Base / SFT / GRPO pipeline &middot; <code>&lt;think&gt;</code> + <code>&lt;step&gt;</code> format</p>

<!-- Overview Stats -->
<div class="summary">
    <h2>Evaluation Overview</h2>
    <p style="font-size:0.88rem; color:#555; margin-bottom:1rem;">
        Two thinking-capable model families evaluated across 7 benchmarks under 3 training conditions.
        <strong>SFT</strong> provides format warmup on verified reasoning traces.
        <strong>GRPO</strong> adds SAT-based consistency reward on top of SFT, producing the full RAILS pipeline.
    </p>
    <div class="stats">
        <div class="stat"><div class="stat-value">2</div><div class="stat-label">Models</div></div>
        <div class="stat stat-green"><div class="stat-value">7</div><div class="stat-label">Benchmarks</div></div>
        <div class="stat stat-purple"><div class="stat-value">3</div><div class="stat-label">Training Conditions</div></div>
    </div>
    <div class="legend">
        <div class="legend-item"><div class="legend-dot" style="background:#94a3b8"></div> Base (no training)</div>
        <div class="legend-item"><div class="legend-dot" style="background:#f59e0b"></div> SFT (format warmup)</div>
        <div class="legend-item"><div class="legend-dot" style="background:#22c55e"></div> GRPO (RAILS &mdash; SAT consistency reward)</div>
    </div>
</div>

<!-- Main Accuracy Table -->
<div class="card">
    <h2>In-Domain Benchmark Accuracy</h2>
    <p class="section-label">Higher is better &middot; Best per model in bold green</p>
    <div class="table-wrapper">
    <table>
        <thead>
            <tr>
                <th style="min-width:160px">Model / Condition</th>
                <th>LogicBench</th>
                <th>FOLIO</th>
                <th>ProntoQA</th>
                <th>ProofWriter</th>
                <th>ACPBench</th>
            </tr>
        </thead>
        <tbody>
            <!-- DeepSeek -->
            <tr class="model-header"><td colspan="6">DeepSeek-R1 (7B)</td></tr>
            <tr class="condition-row"><td>Base</td><td>0.198</td><td>0.059</td><td>0.013</td><td>0.338</td><td>0.001</td></tr>
            <tr class="condition-row"><td>SFT</td><td>0.082</td><td>0.324</td><td>0.030</td><td>0.355</td><td>0.000</td></tr>
            <tr class="condition-row improved"><td>GRPO (RAILS)</td><td class="best">0.284 <span class="delta-pos">+0.086</span></td><td class="best">0.431 <span class="delta-pos">+0.372</span></td><td class="best">0.053</td><td class="best">0.380</td><td>0.000</td></tr>

            <!-- Qwen -->
            <tr class="model-header"><td colspan="6">Qwen3 (8B)</td></tr>
            <tr class="condition-row"><td>Base</td><td>0.080</td><td>0.093</td><td>0.000</td><td>0.378</td><td>0.007</td></tr>
            <tr class="condition-row"><td>SFT</td><td>0.039</td><td>0.343</td><td>0.003</td><td>0.368</td><td>0.002</td></tr>
            <tr class="condition-row improved"><td>GRPO (RAILS)</td><td class="best">0.359 <span class="delta-pos">+0.279</span></td><td class="best">0.402 <span class="delta-pos">+0.309</span></td><td class="best">0.073</td><td class="best">0.385</td><td>0.001</td></tr>
        </tbody>
    </table>
    </div>
    <div class="finding finding-insight" style="margin-top:1rem;">
        <strong>Note on SFT LogicBench dip:</strong> SFT teaches the <code>&lt;think&gt;</code> + <code>&lt;step&gt;</code> output format, which temporarily reduces LogicBench accuracy
        (models must now produce structured proofs instead of free-text answers). GRPO then recovers and surpasses base accuracy
        while maintaining structured output. FOLIO shows immediate and sustained improvement from SFT onward.
    </div>
</div>

<!-- Transfer Benchmarks -->
<div class="card">
    <h2>Transfer Benchmarks &mdash; Out-of-Domain</h2>
    <p class="section-label">Zero-shot transfer to legal reasoning (SARA) and constraint satisfaction (LSAT) &middot; CoT accuracy &middot; <a href="transfer_eval.html" style="color:#2563eb;">Full analysis &rarr;</a></p>
    <div class="table-wrapper">
    <table>
        <thead>
            <tr>
                <th style="min-width:160px">Model / Condition</th>
                <th>SARA CoT</th>
                <th>LSAT CoT</th>
            </tr>
        </thead>
        <tbody>
            <tr class="model-header"><td colspan="3">DeepSeek-R1 (7B)</td></tr>
            <tr class="condition-row"><td>Base</td><td>0.350</td><td>0.152</td></tr>
            <tr class="condition-row"><td>SFT</td><td>0.050</td><td>0.091</td></tr>
            <tr class="condition-row improved"><td>GRPO (RAILS)</td><td class="best">0.400</td><td class="best">0.170</td></tr>

            <tr class="model-header"><td colspan="3">Qwen3 (8B)</td></tr>
            <tr class="condition-row"><td>Base</td><td>0.160</td><td>0.217</td></tr>
            <tr class="condition-row"><td>SFT</td><td>0.110</td><td>0.161</td></tr>
            <tr class="condition-row improved"><td>GRPO (RAILS)</td><td class="best">0.430</td><td class="best">0.283</td></tr>
        </tbody>
    </table>
    </div>
    <div class="finding finding-caution" style="margin-top:1rem;">
        <strong>SFT hurts transfer.</strong> SFT alone consistently degrades out-of-domain performance
        (DeepSeek SARA: 35.0% base &rarr; 5.0% SFT; Qwen LSAT: 21.7% base &rarr; 16.1% SFT).
        GRPO recovers and exceeds base performance, suggesting the consistency reward &mdash; not format training &mdash; drives generalization.
    </div>
</div>

<!-- LogicBench Per-Theory Breakdown -->
<div class="card">
    <h2>LogicBench &mdash; Per-Theory Breakdown (GRPO)</h2>
    <p class="section-label">Propositional logic theories &middot; Best GRPO checkpoints</p>
    <div class="table-wrapper">
    <table>
        <thead>
            <tr>
                <th rowspan="2" style="min-width:180px">Theory</th>
                <th colspan="2">DeepSeek-R1 (7B)</th>
                <th colspan="2">Qwen3 (8B)</th>
            </tr>
            <tr>
                <th>Acc</th>
                <th>Cons.</th>
                <th>Acc</th>
                <th>Cons.</th>
            </tr>
        </thead>
        <tbody>
            <tr><td style="text-align:left;">Disjunctive Syllogism</td><td class="best">0.850</td><td>0.775</td><td class="best">0.800</td><td>0.113</td></tr>
            <tr><td style="text-align:left;">Modus Tollens</td><td>0.500</td><td>0.650</td><td class="best">0.700</td><td>0.100</td></tr>
            <tr><td style="text-align:left;">Hypothetical Syllogism</td><td class="best">0.675</td><td>0.650</td><td>0.438</td><td>0.092</td></tr>
            <tr><td style="text-align:left;">Material Implication</td><td>0.125</td><td>0.306</td><td class="best">0.500</td><td>0.238</td></tr>
            <tr><td style="text-align:left;">Constructive Dilemma</td><td>0.013</td><td>0.063</td><td class="best">0.425</td><td>0.125</td></tr>
            <tr><td style="text-align:left;">Commutation</td><td>0.113</td><td>0.219</td><td class="best">0.225</td><td>0.088</td></tr>
            <tr><td style="text-align:left;">Destructive Dilemma</td><td>0.000</td><td>0.200</td><td class="best">0.163</td><td>0.000</td></tr>
            <tr><td style="text-align:left;">Bidirectional Dilemma</td><td>0.000</td><td>0.250</td><td>0.013</td><td>0.000</td></tr>
        </tbody>
    </table>
    </div>
    <div class="finding finding-insight" style="margin-top:1rem;">
        <strong>Theory-specific strengths differ between models.</strong>
        DeepSeek excels at hypothetical syllogism (0.675) and has higher consistency overall.
        Qwen shows stronger accuracy on material implication (0.500) and constructive dilemma (0.425).
        Both models achieve 0.70+ on simple inference types (disjunctive syllogism, modus tollens)
        but struggle with complex dilemmas (&lt;0.25).
    </div>
</div>

<!-- Training Dynamics -->
<div class="card">
    <h2>Training Dynamics</h2>
    <p class="section-label">How rewards evolve during GRPO training (DeepSeek-R1 7B, beta=0, from SFT checkpoint)</p>
    <div class="table-wrapper">
    <table>
        <thead>
            <tr>
                <th>Step</th>
                <th>Total Reward</th>
                <th>Consistency</th>
                <th>Format</th>
                <th>Answer</th>
                <th>Grad Norm</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>40</td><td>0.20</td><td>-0.03</td><td>0.24</td><td>0.54</td><td>8.1</td></tr>
            <tr><td>90</td><td>0.59</td><td>+0.08</td><td>0.27</td><td>0.61</td><td>5.2</td></tr>
            <tr><td>180</td><td>1.63</td><td>+0.52</td><td>0.51</td><td>0.81</td><td>2.4</td></tr>
            <tr><td>300</td><td>1.95</td><td>+0.52</td><td>0.66</td><td>0.89</td><td>1.3</td></tr>
            <tr><td>600</td><td>2.40</td><td>+0.82</td><td>0.73</td><td>0.94</td><td>1.2</td></tr>
            <tr class="improved"><td>1030</td><td class="best">2.66</td><td class="best">+0.84</td><td class="best">0.77</td><td class="best">1.00</td><td>0.8</td></tr>
        </tbody>
    </table>
    </div>
    <div class="finding finding-positive" style="margin-top:1rem;">
        <strong>Stable training with beta=0.</strong> With SFT warm-start and multi-signal reward (consistency + format + answer + length),
        KL penalty is unnecessary. Consistency improves monotonically from -0.03 to +0.84.
        Gradient norms decrease throughout, indicating stable convergence. See <a href="research_notes.html#beta-zero" style="color:#2563eb;">beta=0 research note</a> for details.
    </div>
</div>

<!-- Key Findings -->
<div class="card">
    <h2>Key Findings</h2>

    <div class="finding finding-positive">
        <strong>GRPO with consistency reward produces the largest gains.</strong>
        Qwen3 improves from 0.080 to 0.359 on LogicBench (+349%). Both models show 3&ndash;7x improvement on FOLIO.
        The consistency signal teaches models to produce structured, terminating proofs.
    </div>

    <div class="finding finding-positive">
        <strong>SFT warmup is essential for format learning.</strong>
        Without SFT, thinking models cannot produce the structured <code>&lt;step&gt;</code> format.
        SFT teaches the output format; GRPO then improves reasoning quality within that format.
    </div>

    <div class="finding finding-insight">
        <strong>GRPO generalizes to unseen domains.</strong>
        On SARA (legal reasoning) and LSAT (constraint satisfaction), GRPO models outperform both base and SFT models.
        Qwen3 GRPO reaches 43.0% on SARA (vs 16.0% base) and 28.3% on LSAT (vs 21.7% base).
    </div>

    <div class="finding finding-caution">
        <strong>SFT alone hurts out-of-domain transfer.</strong>
        SFT models perform worse than base on SARA and LSAT, likely due to format overfitting on logic-specific training data.
        The consistency reward during GRPO prevents this overfitting by rewarding valid reasoning regardless of domain.
    </div>

    <div class="finding finding-insight">
        <strong>Models show complementary strengths.</strong>
        DeepSeek excels at theories requiring chain-building (hypothetical syllogism: 0.675) with high consistency (34% overall).
        Qwen achieves higher accuracy (LogicBench 0.359 vs 0.284) but lower faithfulness (16% vs 91%),
        suggesting different strategies for leveraging the consistency signal.
    </div>
</div>

<!-- Conditions Legend -->
<div class="card">
    <h2>Training Conditions</h2>
    <div style="display:grid; grid-template-columns: repeat(auto-fit, minmax(240px, 1fr)); gap:0.8rem;">
        <div style="padding:0.8rem; background:#f9fafb; border-radius:8px; border-left:3px solid #94a3b8;">
            <strong style="font-size:0.85rem;">Base</strong>
            <p style="font-size:0.8rem; color:#555; margin-top:0.3rem;">Pretrained thinking model with no additional training. Evaluated zero-shot with the structured reasoning prompt.</p>
        </div>
        <div style="padding:0.8rem; background:#f9fafb; border-radius:8px; border-left:3px solid #f59e0b;">
            <strong style="font-size:0.85rem;">SFT (Format Warmup)</strong>
            <p style="font-size:0.8rem; color:#555; margin-top:0.3rem;">Supervised fine-tuning on 6,033 verified reasoning traces in <code>&lt;think&gt;</code> + <code>&lt;step&gt;</code> format. Teaches the model the output structure before RL. QLoRA, 10 epochs.</p>
        </div>
        <div style="padding:0.8rem; background:#f9fafb; border-radius:8px; border-left:3px solid #22c55e;">
            <strong style="font-size:0.85rem;">GRPO (RAILS)</strong>
            <p style="font-size:0.8rem; color:#555; margin-top:0.3rem;">Group Relative Policy Optimization from SFT checkpoint with SAT consistency reward. Each derived step is checked by a SAT solver. Invalid reasoning chains receive zero answer credit.</p>
        </div>
    </div>
</div>

<div class="footer">
    RAILS (Reasoning with Automatically Integrated Logical Supervision) &middot; Updated Feb 2026<br>
    <a href="qualitative_comparison.html" style="color:#2563eb; text-decoration:none; font-weight:500;">View Qualitative Analysis &rarr;</a>
    &middot;
    <a href="transfer_eval.html" style="color:#2563eb; text-decoration:none; font-weight:500;">View Transfer Evaluation &rarr;</a>
</div>
</body>
</html>
