WARNING: Environment variable CUDA_VISIBLE_DEVICES already has value [], will not forward new value [0] from parent process environment
/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info
wandb: Currently logged in as: hkrsnd to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /mnt/vast/home/hs40vahe/ProofContinuity/wandb/run-20260220_221237-xe9lhl73
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deepseek-14b-sat-think-mixed-85860
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hkrsnd/LoGRPO-Experiments
wandb: üöÄ View run at https://wandb.ai/hkrsnd/LoGRPO-Experiments/runs/xe9lhl73
wandb: Detected [huggingface_hub.inference, openai, verifiers] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.14it/s]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:02<00:01,  1.19s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.03s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.04s/it]
Unsloth 2025.9.4 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.
Map:   0%|          | 0/4808 [00:00<?, ? examples/s]Map:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2927/4808 [00:00<00:00, 29144.19 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4808/4808 [00:00<00:00, 27368.40 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:00<00:00, 75.37it/s]

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.22it/s]
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.10it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.17it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.16it/s]

Capturing CUDA graph shapes:   0%|          | 0/5 [00:00<?, ?it/s]Capturing CUDA graph shapes:  20%|‚ñà‚ñà        | 1/5 [00:00<00:02,  1.43it/s]Capturing CUDA graph shapes:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:01<00:01,  1.65it/s]Capturing CUDA graph shapes:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:01<00:01,  1.74it/s]Capturing CUDA graph shapes:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:02<00:00,  1.79it/s]Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.88it/s]Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.79it/s]
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 4,808 | Num Epochs = 2 | Total steps = 2,404
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16
 "-____-"     Trainable parameters = 68,812,800 of 14,907,659,264 (0.46% trained)
  0%|          | 0/2404 [00:00<?, ?it/s]/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:397: UserWarning: Unmerge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/2404 [02:54<116:20:44, 174.30s/it]/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:397: UserWarning: Unmerge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
  0%|          | 2/2404 [06:10<124:47:02, 187.02s/it]  0%|          | 3/2404 [07:30<92:05:12, 138.07s/it]   0%|          | 4/2404 [10:11<98:05:44, 147.14s/it]  0%|          | 5/2404 [12:25<94:59:31, 142.55s/it]  0%|          | 6/2404 [15:26<103:38:24, 155.59s/it]  0%|          | 7/2404 [17:02<90:41:58, 136.22s/it]   0%|          | 8/2404 [20:18<103:15:48, 155.15s/it]  0%|          | 9/2404 [22:37<99:47:42, 150.01s/it]   0%|          | 10/2404 [24:09<87:55:36, 132.22s/it]                                                       0%|          | 10/2404 [24:09<87:55:36, 132.22s/it]  0%|          | 11/2404 [26:55<94:46:57, 142.59s/it]  0%|          | 12/2404 [29:05<92:10:57, 138.74s/it]  1%|          | 13/2404 [31:02<87:48:46, 132.22s/it]  1%|          | 14/2404 [32:29<78:33:48, 118.34s/it]  1%|          | 15/2404 [33:35<68:14:22, 102.83s/it]  1%|          | 16/2404 [35:35<71:35:38, 107.93s/it]  1%|          | 17/2404 [37:19<70:46:47, 106.75s/it]  1%|          | 18/2404 [38:34<64:25:51, 97.21s/it]   1%|          | 19/2404 [40:39<69:54:24, 105.52s/it]  1%|          | 20/2404 [42:21<69:13:55, 104.55s/it]                                                       1%|          | 20/2404 [42:21<69:13:55, 104.55s/it]  1%|          | 21/2404 [45:26<85:11:02, 128.69s/it]  1%|          | 22/2404 [48:40<97:59:52, 148.11s/it]/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/vast/home/hs40vahe/.local/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:397: UserWarning: Unmerge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
  1%|          | 23/2404 [50:15<87:23:28, 132.13s/it]  1%|          | 24/2404 [52:08<83:38:52, 126.53s/it]  1%|          | 25/2404 [53:25<73:48:50, 111.70s/it]  1%|          | 26/2404 [56:33<88:48:01, 134.43s/it]  1%|          | 27/2404 [58:35<86:27:06, 130.93s/it]  1%|          | 28/2404 [1:00:57<88:33:33, 134.18s/it]  1%|          | 29/2404 [1:04:03<98:41:58, 149.61s/it]  1%|          | 30/2404 [1:06:38<99:51:42, 151.43s/it]                                                         1%|          | 30/2404 [1:06:38<99:51:42, 151.43s/it]  1%|‚ñè         | 31/2404 [1:09:06<99:05:32, 150.33s/it]  1%|‚ñè         | 32/2404 [1:11:38<99:18:42, 150.73s/it]  1%|‚ñè         | 33/2404 [1:13:02<86:10:55, 130.85s/it]  1%|‚ñè         | 34/2404 [1:14:27<76:58:08, 116.92s/it]  1%|‚ñè         | 35/2404 [1:17:19<87:48:56, 133.45s/it]  1%|‚ñè         | 36/2404 [1:19:56<92:27:48, 140.57s/it]  2%|‚ñè         | 37/2404 [1:22:20<93:05:04, 141.57s/it]  2%|‚ñè         | 38/2404 [1:24:39<92:39:18, 140.98s/it]  2%|‚ñè         | 39/2404 [1:25:52<79:08:26, 120.47s/it]  2%|‚ñè         | 40/2404 [1:28:03<81:06:59, 123.53s/it]                                                         2%|‚ñè         | 40/2404 [1:28:03<81:06:59, 123.53s/it]  2%|‚ñè         | 41/2404 [1:31:16<94:44:15, 144.33s/it]  2%|‚ñè         | 42/2404 [1:33:15<89:51:47, 136.96s/it]  2%|‚ñè         | 43/2404 [1:36:11<97:28:13, 148.62s/it]  2%|‚ñè         | 44/2404 [1:37:47<87:07:13, 132.90s/it]  2%|‚ñè         | 45/2404 [1:41:02<99:10:43, 151.35s/it]  2%|‚ñè         | 46/2404 [1:42:34<87:32:49, 133.66s/it]  2%|‚ñè         | 47/2404 [1:45:02<90:14:59, 137.84s/it]  2%|‚ñè         | 48/2404 [1:47:10<88:20:32, 134.99s/it]  2%|‚ñè         | 49/2404 [1:49:34<89:59:52, 137.58s/it]  2%|‚ñè         | 50/2404 [1:51:04<80:36:47, 123.28s/it]                                                         2%|‚ñè         | 50/2404 [1:51:05<80:36:47, 123.28s/it]  2%|‚ñè         | 51/2404 [1:53:06<80:24:01, 123.01s/it]  2%|‚ñè         | 52/2404 [1:54:24<71:37:53, 109.64s/it]  2%|‚ñè         | 53/2404 [1:56:44<77:29:58, 118.67s/it]  2%|‚ñè         | 54/2404 [1:59:13<83:18:20, 127.62s/it]  2%|‚ñè         | 55/2404 [2:00:50<77:22:45, 118.59s/it]  2%|‚ñè         | 56/2404 [2:03:00<79:34:18, 122.00s/it]  2%|‚ñè         | 57/2404 [2:05:45<87:53:42, 134.82s/it]  2%|‚ñè         | 58/2404 [2:07:54<86:38:30, 132.95s/it]  2%|‚ñè         | 59/2404 [2:10:15<88:13:35, 135.44s/it]  2%|‚ñè         | 60/2404 [2:13:20<97:57:59, 150.46s/it]                                                         2%|‚ñè         | 60/2404 [2:13:20<97:57:59, 150.46s/it]  3%|‚ñé         | 61/2404 [2:16:01<99:52:21, 153.45s/it]  3%|‚ñé         | 62/2404 [2:16:52<79:50:01, 122.72s/it]  3%|‚ñé         | 63/2404 [2:20:04<93:19:50, 143.52s/it]  3%|‚ñé         | 64/2404 [2:23:13<102:12:35, 157.25s/it]  3%|‚ñé         | 65/2404 [2:26:26<109:03:29, 167.85s/it]  3%|‚ñé         | 66/2404 [2:28:43<102:59:10, 158.58s/it]  3%|‚ñé         | 67/2404 [2:31:18<102:14:56, 157.51s/it]  3%|‚ñé         | 68/2404 [2:34:34<109:47:59, 169.21s/it]  3%|‚ñé         | 69/2404 [2:36:52<103:39:05, 159.81s/it]  3%|‚ñé         | 70/2404 [2:39:48<106:49:56, 164.78s/it]                                                          3%|‚ñé         | 70/2404 [2:39:48<106:49:56, 164.78s/it]  3%|‚ñé         | 71/2404 [2:42:22<104:41:24, 161.54s/it]  3%|‚ñé         | 72/2404 [2:45:41<111:56:54, 172.82s/it]  3%|‚ñé         | 73/2404 [2:48:49<114:45:54, 177.24s/it]  3%|‚ñé         | 74/2404 [2:52:04<118:11:59, 182.63s/it]  3%|‚ñé         | 75/2404 [2:54:58<116:26:06, 179.98s/it]  3%|‚ñé         | 76/2404 [2:57:59<116:32:26, 180.22s/it]  3%|‚ñé         | 77/2404 [3:01:16<119:50:22, 185.40s/it]  3%|‚ñé         | 78/2404 [3:04:36<122:31:29, 189.63s/it]  3%|‚ñé         | 79/2404 [3:07:54<124:08:21, 192.22s/it]  3%|‚ñé         | 80/2404 [3:11:11<125:05:03, 193.76s/it]                                                          3%|‚ñé         | 80/2404 [3:11:11<125:05:03, 193.76s/it]  3%|‚ñé         | 81/2404 [3:14:26<125:08:30, 193.93s/it]  3%|‚ñé         | 82/2404 [3:17:41<125:23:47, 194.41s/it]slurmstepd: error: *** JOB 85860 ON cn04 CANCELLED AT 2026-02-21T01:33:31 ***
