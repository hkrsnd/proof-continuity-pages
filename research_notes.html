<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RAILS — Research Notes</title>
<style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body { font-family: 'Segoe UI', system-ui, -apple-system, sans-serif; background: #f8f9fa; color: #1a1a2e; line-height: 1.6; padding: 2rem; max-width: 960px; margin: 0 auto; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    .back { display: inline-block; margin-bottom: 1.5rem; font-size: 0.9rem; color: #64748b; }
    .back:hover { color: #2563eb; }
    h1 { font-size: 1.8rem; margin-bottom: 0.3rem; color: #16213e; }
    .subtitle { color: #666; font-size: 0.95rem; margin-bottom: 2rem; }

    /* TOC */
    .toc { background: white; border-radius: 12px; padding: 1.5rem; margin-bottom: 2rem; box-shadow: 0 1px 3px rgba(0,0,0,0.08); }
    .toc h2 { font-size: 1rem; color: #16213e; margin-bottom: 0.8rem; }
    .toc-item { display: flex; align-items: baseline; gap: 0.8rem; padding: 0.5rem 0; border-bottom: 1px solid #f0f0f0; }
    .toc-item:last-child { border-bottom: none; }
    .toc-date { font-size: 0.78rem; color: #94a3b8; white-space: nowrap; min-width: 80px; }
    .toc-link { font-size: 0.92rem; font-weight: 500; }
    .toc-desc { font-size: 0.82rem; color: #666; margin-top: 0.15rem; }
    .toc-tag { display: inline-block; padding: 0.1rem 0.45rem; border-radius: 20px; font-size: 0.68rem; font-weight: 600; margin-left: 0.4rem; }
    .tag-red { background: #fee2e2; color: #991b1b; }
    .tag-blue { background: #e0e7ff; color: #3730a3; }
    .tag-amber { background: #fef3c7; color: #92400e; }
    .tag-green { background: #dcfce7; color: #166534; }

    /* Note cards */
    .note { background: white; border-radius: 12px; margin-bottom: 2rem; box-shadow: 0 1px 3px rgba(0,0,0,0.08); overflow: hidden; }
    .note-header { padding: 1.2rem 1.5rem; cursor: pointer; display: flex; align-items: center; gap: 0.8rem; border-bottom: 1px solid #f0f0f0; user-select: none; }
    .note-header:hover { background: #fafbfc; }
    .note-chevron { font-size: 0.8rem; color: #94a3b8; transition: transform 0.2s; flex-shrink: 0; }
    .note.open .note-chevron { transform: rotate(90deg); }
    .note-title { font-size: 1.05rem; font-weight: 600; color: #16213e; flex: 1; }
    .note-date { font-size: 0.78rem; color: #94a3b8; white-space: nowrap; }
    .note-body { padding: 1.5rem; display: none; }
    .note.open .note-body { display: block; }

    /* Content styling */
    .note-body h2 { font-size: 1.1rem; color: #16213e; margin: 1.5rem 0 0.6rem 0; padding-bottom: 0.3rem; border-bottom: 1px solid #e5e7eb; }
    .note-body h2:first-child { margin-top: 0; }
    .note-body h3 { font-size: 0.95rem; color: #334155; margin: 1.2rem 0 0.4rem 0; }
    .note-body p { font-size: 0.88rem; color: #444; margin-bottom: 0.8rem; }
    .note-body ul, .note-body ol { font-size: 0.88rem; color: #444; margin: 0.4rem 0 0.8rem 1.5rem; }
    .note-body li { margin-bottom: 0.3rem; }
    .note-body strong { color: #1a1a2e; }
    .note-body code { background: #f1f5f9; padding: 0.15rem 0.4rem; border-radius: 4px; font-size: 0.82rem; font-family: 'SF Mono', 'Fira Code', monospace; color: #e11d48; }
    .note-body pre { background: #1e293b; color: #e2e8f0; padding: 1rem; border-radius: 8px; overflow-x: auto; margin: 0.6rem 0 1rem 0; font-size: 0.8rem; line-height: 1.5; font-family: 'SF Mono', 'Fira Code', monospace; }
    .note-body pre code { background: none; color: inherit; padding: 0; font-size: inherit; }

    /* Tables */
    .note-body table { width: 100%; border-collapse: collapse; margin: 0.6rem 0 1rem 0; font-size: 0.82rem; }
    .note-body th { background: #f8fafc; color: #334155; font-weight: 600; text-align: left; padding: 0.5rem 0.6rem; border-bottom: 2px solid #e2e8f0; }
    .note-body td { padding: 0.45rem 0.6rem; border-bottom: 1px solid #f0f0f0; color: #444; }
    .note-body tr:hover td { background: #fafbfc; }

    /* Highlight boxes */
    .finding { background: #eff6ff; border-left: 3px solid #3b82f6; padding: 0.8rem 1rem; border-radius: 0 8px 8px 0; margin: 0.8rem 0; font-size: 0.86rem; }
    .finding.warn { background: #fef3c7; border-left-color: #f59e0b; }
    .finding.good { background: #dcfce7; border-left-color: #16a34a; }
    .finding.bad { background: #fee2e2; border-left-color: #dc2626; }
    .finding strong { color: #1e3a5f; }

    /* Sample outputs */
    .sample { background: #f9fafb; border: 1px solid #e5e7eb; border-radius: 8px; padding: 1rem; margin: 0.6rem 0 1rem 0; font-size: 0.82rem; font-family: 'SF Mono', monospace; white-space: pre-wrap; line-height: 1.5; overflow-x: auto; }
    .sample .good-tag { color: #16a34a; font-weight: 600; }
    .sample .bad-tag { color: #dc2626; font-weight: 600; }
    .sample .garbled { background: #fee2e2; padding: 0.1rem 0.3rem; border-radius: 3px; }

    .footer { text-align: center; color: #94a3b8; font-size: 0.8rem; margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e5e7eb; }
</style>
</head>
<body>

<a class="back" href="index.html">&larr; Back to Hub</a>
<h1>Research Notes</h1>
<p class="subtitle">Findings, analyses, and design decisions from the RAILS project</p>

<!-- Table of Contents -->
<div class="toc">
    <h2>Contents</h2>
    <div class="toc-item">
        <span class="toc-date">2026-02-21</span>
        <div>
            <a class="toc-link" href="#status-feb21">Experiment Status &amp; Key Insights (Feb 21)</a>
            <span class="toc-tag tag-red">key finding</span>
            <span class="toc-tag tag-green">status</span>
            <div class="toc-desc">Reward-eval disconnect (highest reward = worst eval), SFT J-curve regression, full leaderboard across 17+ models and 6 benchmarks, per-theory analysis, 11 active SLURM jobs.</div>
        </div>
    </div>
    <div class="toc-item">
        <span class="toc-date">2026-02-21</span>
        <div>
            <a class="toc-link" href="#beta-zero">The Beta=0 Discovery: Why KL Penalty Destroys GRPO Training</a>
            <span class="toc-tag tag-red">key finding</span>
            <div class="toc-desc">Reference run had NaN KL throughout &mdash; effectively no penalty. Setting beta=0 explicitly produced the best results ever: reward 2.66, consistency 0.84, format compliance 68%.</div>
        </div>
    </div>
    <div class="toc-item">
        <span class="toc-date">2026-02-21</span>
        <div>
            <a class="toc-link" href="#think-template">Think Template Divergence: DeepSeek-R1 vs Qwen3/QwQ</a>
            <span class="toc-tag tag-red">key finding</span>
            <div class="toc-desc">Why think-stripping models resist structured verification training. Empirical evidence, root cause, and recommendations.</div>
        </div>
    </div>
    <div class="toc-item">
        <span class="toc-date">2026-02-20</span>
        <div>
            <a class="toc-link" href="#nli-vs-sat">NLI vs SAT: Why Formal Verification Beats Soft Approximation</a>
            <span class="toc-tag tag-blue">motivation</span>
            <div class="toc-desc">Head-to-head comparison on 5 real invalid reasoning chains. NLI false-accepts 60%, SAT rejects 100%.</div>
        </div>
    </div>
    <div class="toc-item">
        <span class="toc-date">2026-02-20</span>
        <div>
            <a class="toc-link" href="#real-world">Connecting RAILS to Science and Law</a>
            <span class="toc-tag tag-amber">paper framing</span>
            <div class="toc-desc">Benchmarks, related work, and motivation for why formal verification matters in high-stakes domains.</div>
        </div>
    </div>
</div>

<!-- ═══════════════════════════════════════════════════════════════════════ -->
<!-- NOTE: Experiment Status & Key Insights (Feb 21)                        -->
<!-- ═══════════════════════════════════════════════════════════════════════ -->
<div class="note open" id="status-feb21">
    <div class="note-header" onclick="this.parentElement.classList.toggle('open')">
        <span class="note-chevron">&#x25B6;</span>
        <span class="note-title">Experiment Status &amp; Key Insights (Feb 21)</span>
        <span class="note-date">2026-02-21</span>
    </div>
    <div class="note-body">

        <div class="finding bad">
            <strong>Critical finding:</strong> Training reward does NOT predict downstream eval performance. Run 85934 (beta=0, highest training reward <strong>2.77</strong>) has the <strong>worst</strong> eval (LogicBench = 5.0%). Run 85931 (reward 1.25) has the <strong>best</strong> DeepSeek eval (LogicBench = 28.4%). This is strong evidence of reward hacking.
        </div>

        <!-- ── Section 1: Reward-Eval Disconnect ── -->
        <h2>1. Reward-Eval Disconnect</h2>

        <p>Comparing training reward to LogicBench evaluation accuracy reveals a striking inverse relationship for DeepSeek runs:</p>

        <table>
            <tr><th>Run</th><th>Config</th><th>Train Reward</th><th>LB Acc</th><th>Consistency</th><th>Faithfulness</th></tr>
            <tr><td>85934</td><td>beta=0, LR=2e-5 (v3)</td><td style="color:#16a34a;font-weight:700">2.77</td><td style="color:#dc2626;font-weight:700">5.0%</td><td>0.127</td><td>0.641</td></tr>
            <tr><td>85855</td><td>beta=0.04, LR=5e-5 (v1)</td><td>2.66</td><td>11.4%</td><td>0.139</td><td>0.648</td></tr>
            <tr><td>85781</td><td>beta=0.04, LR=5e-5 (v1)</td><td>2.40</td><td>13.0%</td><td>0.340</td><td>0.909</td></tr>
            <tr style="background:#f0fdf4"><td>85931</td><td>beta=0.04, LR=2e-5 (v2)</td><td style="color:#b45309">1.25</td><td style="color:#16a34a;font-weight:700">28.4%</td><td style="color:#16a34a;font-weight:700">0.389</td><td style="color:#16a34a;font-weight:700">0.907</td></tr>
        </table>

        <div class="finding warn">
            <strong>Pattern:</strong> The runs with the highest training reward have collapsed into a narrow output mode &mdash; they score well on training examples but fail to generalize. Run 85931 (moderate reward, lower LR) retains more diverse reasoning strategies. This is classic <strong>reward hacking</strong>: the model finds a shortcut that satisfies the reward function without actually learning the target skill.
        </div>

        <p>For Qwen3, the same pattern holds but the gap is smaller:</p>
        <table>
            <tr><th>Run</th><th>Train Reward</th><th>LB Acc</th><th>Consistency</th><th>Faithfulness</th></tr>
            <tr style="background:#f0fdf4"><td>85716 (GRPO)</td><td>2.22</td><td style="color:#16a34a;font-weight:700">35.9%</td><td>0.093</td><td>0.159</td></tr>
            <tr><td>85307 (diff-anneal)</td><td>1.54</td><td>6.1%</td><td>0.000</td><td>0.245</td></tr>
            <tr><td>85303 (SAT mixed)</td><td>1.68</td><td>8.7%</td><td>0.000</td><td>0.242</td></tr>
        </table>

        <!-- ── Section 2: SFT Regression (J-Curve) ── -->
        <h2>2. The SFT J-Curve</h2>

        <p>SFT warm-start alone makes models <strong>worse</strong> on LogicBench. But GRPO then recovers dramatically &mdash; a classic J-shaped learning curve.</p>

        <table>
            <tr><th>Model</th><th>Stage</th><th>LogicBench</th><th>FOLIO</th><th>ProntoQA</th><th>ProofWriter</th></tr>
            <tr><td rowspan="3"><strong>DeepSeek-7B</strong></td><td>Base</td><td>15.9%</td><td>5.9%</td><td>1.3%</td><td>33.8%</td></tr>
            <tr><td style="color:#dc2626">SFT (85831)</td><td style="color:#dc2626">8.2% <small>(-48%)</small></td><td>6.4%</td><td>0.0%</td><td>35.2%</td></tr>
            <tr style="background:#f0fdf4"><td style="color:#16a34a">GRPO (85931)</td><td style="color:#16a34a;font-weight:700">28.4% <small>(+246%)</small></td><td>&#8212;</td><td>&#8212;</td><td>&#8212;</td></tr>
            <tr style="border-top:2px solid #e2e8f0"><td rowspan="3"><strong>Qwen3-8B</strong></td><td>Base</td><td>6.1%</td><td>9.3%</td><td>0.0%</td><td>37.8%</td></tr>
            <tr><td style="color:#dc2626">SFT (85832)</td><td style="color:#dc2626">3.9% <small>(-36%)</small></td><td>10.3%</td><td>0.0%</td><td>36.5%</td></tr>
            <tr style="background:#f0fdf4"><td style="color:#16a34a">GRPO (85716)</td><td style="color:#16a34a;font-weight:700">35.9% <small>(+820%)</small></td><td>18.1%</td><td>8.0%</td><td>43.8%</td></tr>
        </table>

        <div class="finding good">
            <strong>Interpretation:</strong> SFT narrows the output distribution into the <code>&lt;step&gt;</code> format but the model doesn't yet know <em>how</em> to reason in that format &mdash; it loses generality. GRPO then refines within this constrained space, learning to produce logically valid chains. The combination is essential: SFT without GRPO hurts; GRPO without SFT fails to adopt the format.
        </div>

        <!-- ── Section 3: Full Eval Leaderboard ── -->
        <h2>3. Comprehensive Eval Leaderboard</h2>

        <h3>LogicBench (Propositional) &mdash; Primary Benchmark</h3>
        <p>Sorted by accuracy. Bold = RAILS-trained models.</p>

        <table>
            <tr><th>Model</th><th>Method</th><th>LB Acc</th><th>Consist.</th><th>Faithful.</th><th>Parse %</th><th>Avg Steps</th></tr>
            <tr><td>Llama-3.1-8B</td><td>Base</td><td>71.2%</td><td>0.403</td><td>0.765</td><td>100%</td><td>4.09</td></tr>
            <tr><td>OLMo-3-7B</td><td>Base</td><td>66.6%</td><td>0.317</td><td>0.252</td><td>99.7%</td><td>3.93</td></tr>
            <tr><td>Mistral-7B</td><td>Base</td><td>50.0%</td><td>0.237</td><td>0.583</td><td>100%</td><td>4.48</td></tr>
            <tr style="background:#f0fdf4"><td><strong>Qwen3-8B</strong></td><td><strong>GRPO (85716)</strong></td><td style="color:#16a34a;font-weight:700">35.9%</td><td>0.093</td><td>0.159</td><td>31.6%</td><td>0.89</td></tr>
            <tr style="background:#f0fdf4"><td><strong>DeepSeek-7B</strong></td><td><strong>GRPO (85931)</strong></td><td style="color:#16a34a;font-weight:700">28.4%</td><td style="color:#16a34a">0.389</td><td style="color:#16a34a">0.907</td><td>95.9%</td><td>2.88</td></tr>
            <tr><td>DeepSeek-7B</td><td>Answer-only</td><td>24.5%</td><td>0.000</td><td>0.484</td><td>48.4%</td><td>0.96</td></tr>
            <tr><td>DeepSeek-7B</td><td>Base</td><td>15.9%</td><td>0.000</td><td>0.423</td><td>42.3%</td><td>1.20</td></tr>
            <tr><td><strong>DeepSeek-7B</strong></td><td>GRPO (85781)</td><td>13.0%</td><td>0.340</td><td>0.909</td><td>100%</td><td>3.30</td></tr>
            <tr><td><strong>DeepSeek-7B</strong></td><td>GRPO (85855)</td><td>11.4%</td><td>0.139</td><td>0.648</td><td>67.5%</td><td>1.66</td></tr>
            <tr><td>Qwen3-8B</td><td>SAT mixed (85303)</td><td>8.7%</td><td>0.000</td><td>0.242</td><td>24.2%</td><td>0.59</td></tr>
            <tr><td>DeepSeek-7B</td><td>SFT (85831)</td><td>8.2%</td><td>0.127</td><td>0.650</td><td>66.6%</td><td>1.55</td></tr>
            <tr><td>Qwen3-8B</td><td>Base</td><td>6.1%</td><td>0.000</td><td>0.230</td><td>23.0%</td><td>0.54</td></tr>
            <tr><td>Qwen3-8B</td><td>Diff-anneal (85307)</td><td>6.1%</td><td>0.000</td><td>0.245</td><td>24.5%</td><td>0.57</td></tr>
            <tr><td><strong>DeepSeek-7B</strong></td><td>GRPO (85934)</td><td>5.0%</td><td>0.127</td><td>0.641</td><td>65.5%</td><td>1.55</td></tr>
            <tr><td>Qwen3-8B</td><td>SFT (85832)</td><td>3.9%</td><td>0.029</td><td>0.064</td><td>7.1%</td><td>0.15</td></tr>
        </table>

        <div class="finding">
            <strong>Note:</strong> Base Llama (71.2%) and OLMo (66.6%) score far higher than any RAILS model on LogicBench &mdash; but they use free-form CoT (classic <code>Step N:</code> format), not the structured <code>&lt;step&gt;</code>+<code>&lt;logic&gt;</code> format. The format constraint is a major handicap that GRPO partially overcomes. DeepSeek GRPO 85931 achieves 90.7% faithfulness vs Llama's 76.5%.
        </div>

        <h3>Transfer Benchmarks</h3>
        <p>Performance on out-of-distribution benchmarks. Sorted by FOLIO accuracy.</p>

        <table>
            <tr><th>Model</th><th>FOLIO</th><th>ProntoQA</th><th>ProofWriter</th><th>ACPBench</th></tr>
            <tr><td>Mistral answer-only</td><td>46.1%</td><td>7.7%</td><td>35.8%</td><td>&#8212;</td></tr>
            <tr><td>OLMo base</td><td>45.6%</td><td>54.7%</td><td>43.3%</td><td>10.7%</td></tr>
            <tr><td>Llama base</td><td>43.6%</td><td>20.0%</td><td>40.3%</td><td>9.4%</td></tr>
            <tr><td>Mistral diff-anneal</td><td>42.6%</td><td>0.7%</td><td>37.0%</td><td>&#8212;</td></tr>
            <tr><td>Mistral SAT</td><td>42.6%</td><td>6.7%</td><td>35.7%</td><td>&#8212;</td></tr>
            <tr><td>Mistral base</td><td>41.2%</td><td>26.3%</td><td>36.5%</td><td>1.6%</td></tr>
            <tr><td>Llama diff-anneal</td><td>39.7%</td><td>17.0%</td><td>39.8%</td><td>&#8212;</td></tr>
            <tr><td>Llama SAT</td><td>33.8%</td><td>17.0%</td><td>40.3%</td><td>&#8212;</td></tr>
            <tr style="background:#f0fdf4"><td><strong>Qwen GRPO (85716)</strong></td><td>18.1%</td><td>8.0%</td><td style="color:#16a34a;font-weight:700">43.8%</td><td>0.1%</td></tr>
            <tr><td>DeepSeek GRPO (85855)</td><td>11.8%</td><td>0.3%</td><td>34.3%</td><td>0.0%</td></tr>
            <tr><td>Qwen base</td><td>9.3%</td><td>0.0%</td><td>37.8%</td><td>0.7%</td></tr>
            <tr><td>DeepSeek SAT mixed</td><td>7.8%</td><td>8.3%</td><td>34.5%</td><td>&#8212;</td></tr>
            <tr><td>DeepSeek answer-only</td><td>6.9%</td><td>0.0%</td><td>34.2%</td><td>0.2%</td></tr>
            <tr><td>DeepSeek base</td><td>5.9%</td><td>1.3%</td><td>33.8%</td><td>0.1%</td></tr>
        </table>

        <h3>SARA &amp; LSAT (Legal Reasoning)</h3>
        <p>Only evaluated for a subset of models. CoT = chain-of-thought prompting.</p>

        <table>
            <tr><th>Model</th><th>SARA</th><th>SARA CoT</th><th>LSAT</th><th>LSAT CoT</th></tr>
            <tr style="background:#f0fdf4"><td><strong>Qwen GRPO (85716)</strong></td><td>4.0%</td><td style="color:#16a34a;font-weight:700">43.0%</td><td>1.3%</td><td style="color:#16a34a;font-weight:700">28.3%</td></tr>
            <tr><td>DeepSeek GRPO (85781)</td><td>0.0%</td><td>40.0%</td><td>0.9%</td><td>17.0%</td></tr>
            <tr><td>DeepSeek base</td><td>2.0%</td><td>35.0%</td><td>0.0%</td><td>15.2%</td></tr>
            <tr><td>Qwen base</td><td>0.0%</td><td>16.0%</td><td>1.3%</td><td>21.7%</td></tr>
            <tr><td>Qwen SFT (85832)</td><td>0.0%</td><td>11.0%</td><td>1.3%</td><td>16.1%</td></tr>
            <tr><td>DeepSeek SFT (85831)</td><td>1.0%</td><td>5.0%</td><td>2.2%</td><td>9.1%</td></tr>
        </table>

        <div class="finding">
            <strong>SARA/LSAT takeaway:</strong> Direct (non-CoT) accuracy is near 0% for all models &mdash; these tasks need multi-step reasoning. With CoT prompting, Qwen GRPO reaches 43% on SARA (tax law) and 28.3% on LSAT. Consistency is effectively 0 on all legal benchmarks (the SAT verifier can't parse legal predicates).
        </div>

        <!-- ── Section 4: Per-Theory Analysis ── -->
        <h2>4. Per-Theory Analysis</h2>

        <p>LogicBench propositional contains 8 theories. Performance varies dramatically by theory difficulty.</p>

        <h3>Best DeepSeek Run (85931) &mdash; Per Theory</h3>
        <table>
            <tr><th>Theory</th><th>Accuracy</th><th>Consistency</th><th>Faithfulness</th><th>Difficulty</th></tr>
            <tr style="background:#f0fdf4"><td>Disjunctive syllogism</td><td style="color:#16a34a;font-weight:700">85.0%</td><td>0.775</td><td>0.850</td><td style="color:#16a34a">Easy</td></tr>
            <tr style="background:#f0fdf4"><td>Hypothetical syllogism</td><td style="color:#16a34a;font-weight:700">67.5%</td><td>0.650</td><td>0.900</td><td style="color:#16a34a">Easy</td></tr>
            <tr><td>Modus tollens</td><td>50.0%</td><td>0.650</td><td>0.950</td><td>Medium</td></tr>
            <tr><td>Material implication</td><td>12.5%</td><td>0.306</td><td>0.900</td><td style="color:#b45309">Hard</td></tr>
            <tr><td>Commutation</td><td>11.3%</td><td>0.219</td><td>0.850</td><td style="color:#b45309">Hard</td></tr>
            <tr><td>Constructive dilemma</td><td>1.3%</td><td>0.062</td><td>0.975</td><td style="color:#dc2626">Very hard</td></tr>
            <tr style="background:#fff5f5"><td>Bidirectional dilemma</td><td style="color:#dc2626;font-weight:700">0.0%</td><td>0.250</td><td>0.850</td><td style="color:#dc2626">Very hard</td></tr>
            <tr style="background:#fff5f5"><td>Destructive dilemma</td><td style="color:#dc2626;font-weight:700">0.0%</td><td>0.200</td><td>0.975</td><td style="color:#dc2626">Very hard</td></tr>
        </table>

        <h3>Best Qwen Run (85716) &mdash; Per Theory</h3>
        <table>
            <tr><th>Theory</th><th>Accuracy</th><th>Consistency</th><th>Faithfulness</th></tr>
            <tr style="background:#f0fdf4"><td>Disjunctive syllogism</td><td style="color:#16a34a;font-weight:700">80.0%</td><td>0.113</td><td>0.175</td></tr>
            <tr style="background:#f0fdf4"><td>Modus tollens</td><td style="color:#16a34a;font-weight:700">70.0%</td><td>0.100</td><td>0.100</td></tr>
            <tr><td>Material implication</td><td>50.0%</td><td>0.237</td><td>0.175</td></tr>
            <tr><td>Hypothetical syllogism</td><td>43.8%</td><td>0.092</td><td>0.225</td></tr>
            <tr><td>Constructive dilemma</td><td>42.5%</td><td>0.125</td><td>0.275</td></tr>
            <tr><td>Commutation</td><td>22.5%</td><td>0.087</td><td>0.175</td></tr>
            <tr><td>Destructive dilemma</td><td>16.3%</td><td>0.000</td><td>0.125</td></tr>
            <tr style="background:#fff5f5"><td>Bidirectional dilemma</td><td style="color:#dc2626;font-weight:700">1.3%</td><td>0.000</td><td>0.000</td></tr>
        </table>

        <div class="finding warn">
            <strong>Theory difficulty spectrum:</strong> Disjunctive syllogism (80&ndash;85%) is near-solved. Bidirectional dilemma (0&ndash;1%) is essentially unsolved. The three dilemma theories (constructive, destructive, bidirectional) all require coordinating multiple conditionals &mdash; fundamentally harder for step-level verification. These 3 theories account for the bulk of the accuracy gap vs. base models.
        </div>

        <h3>DeepSeek vs Qwen: Complementary Strengths</h3>
        <table>
            <tr><th>Theory</th><th>DS 85931</th><th>Qwen 85716</th><th>Winner</th></tr>
            <tr><td>Disjunctive syllogism</td><td>85.0%</td><td>80.0%</td><td>DS (+5)</td></tr>
            <tr><td>Hypothetical syllogism</td><td style="color:#16a34a;font-weight:700">67.5%</td><td>43.8%</td><td style="color:#16a34a">DS (+23.7)</td></tr>
            <tr><td>Modus tollens</td><td>50.0%</td><td style="color:#16a34a;font-weight:700">70.0%</td><td style="color:#16a34a">Qwen (+20)</td></tr>
            <tr><td>Material implication</td><td>12.5%</td><td style="color:#16a34a;font-weight:700">50.0%</td><td style="color:#16a34a">Qwen (+37.5)</td></tr>
            <tr><td>Constructive dilemma</td><td>1.3%</td><td style="color:#16a34a;font-weight:700">42.5%</td><td style="color:#16a34a">Qwen (+41.2)</td></tr>
            <tr><td>Commutation</td><td>11.3%</td><td style="color:#16a34a;font-weight:700">22.5%</td><td>Qwen (+11.2)</td></tr>
        </table>

        <div class="finding">
            <strong>Insight:</strong> DeepSeek excels on chain-based theories (hypothetical syllogism, disjunctive syllogism) where high consistency + faithfulness &gt;90% translates to accuracy. Qwen excels on broader set of theories but with <em>much lower</em> faithfulness (15&ndash;28%) &mdash; it often gets the right answer without valid formal reasoning. An ensemble or theory-adaptive strategy could combine both strengths.
        </div>

        <!-- ── Section 5: Active Experiments ── -->
        <h2>5. Active Experiments</h2>

        <p>11 SLURM jobs running as of Feb 21, 2026. Snapshot from <code>squeue</code>:</p>

        <table>
            <tr><th>Job</th><th>Type</th><th>Model</th><th>Runtime</th><th>Status / Notes</th></tr>
            <tr style="background:#f0fdf4"><td>85934</td><td>GRPO</td><td>DeepSeek-7B (v3, beta=0)</td><td>8h 32m</td><td style="color:#16a34a">Step 1400, reward 2.77 (highest ever)</td></tr>
            <tr style="background:#f0fdf4"><td>85931</td><td>GRPO</td><td>DeepSeek-7B (v2, LR=2e-5)</td><td>8h 52m</td><td style="color:#16a34a">Step 1200, reward 1.25 &mdash; best eval so far</td></tr>
            <tr><td>85954</td><td>GRPO</td><td>DeepSeek-14B (v4, beta=0)</td><td>2h 12m</td><td>Early (step ~100), stable so far</td></tr>
            <tr><td>85855</td><td>GRPO</td><td>DeepSeek-7B (v1, NaN KL)</td><td>1d 3h</td><td>Step 2900, reward 2.66 (reference run)</td></tr>
            <tr><td>85764</td><td>2-GPU GRPO</td><td>QwQ-32B</td><td>1d 14h</td><td>Step 500, reward 0.79 (slow: 230s/step)</td></tr>
            <tr><td>85881</td><td>2-GPU GRPO</td><td>QwQ-32B (#2)</td><td>22h 47m</td><td>Step 300, reward -0.17 (struggling)</td></tr>
            <tr><td>85844_1</td><td>PPO multi-node</td><td>Test (2 nodes)</td><td>1d 4h</td><td>PPO infrastructure test</td></tr>
            <tr><td>85953_1</td><td>PPO multi-node</td><td>Test (2 nodes)</td><td>3h 19m</td><td>PPO infrastructure test</td></tr>
            <tr><td>85963</td><td>vLLM serve</td><td>&#8212;</td><td>39m</td><td>Inference server</td></tr>
            <tr><td>85964</td><td>Agentic lab</td><td>&#8212;</td><td>38m</td><td>Agent session</td></tr>
            <tr><td>85965</td><td>Agentic lab</td><td>&#8212;</td><td>38m</td><td>Agent session</td></tr>
        </table>

        <h3>Top GRPO Runs by Final Reward (All Time)</h3>
        <table>
            <tr><th>Run</th><th>Model</th><th>Steps</th><th>Final Reward</th><th>Eval LB</th></tr>
            <tr><td>85934</td><td>DeepSeek-7B (v3)</td><td>1400 (ongoing)</td><td style="color:#16a34a;font-weight:700">2.77</td><td style="color:#dc2626">5.0%</td></tr>
            <tr><td>85562</td><td>Qwen3-8B</td><td>4808 (done)</td><td>2.64</td><td>&#8212;</td></tr>
            <tr><td>85855</td><td>DeepSeek-7B (v1)</td><td>2900 (ongoing)</td><td>2.66</td><td>11.4%</td></tr>
            <tr><td>85781</td><td>DeepSeek-7B (v1)</td><td>4808 (done)</td><td>2.40</td><td>13.0%</td></tr>
            <tr><td>85716</td><td>Qwen3-8B</td><td>4808 (done)</td><td>2.22</td><td style="color:#16a34a;font-weight:700">35.9%</td></tr>
            <tr><td>85099</td><td>Mistral-7B (SAT)</td><td>4808 (done)</td><td>2.00</td><td>&#8212;</td></tr>
            <tr><td>85304</td><td>Llama-8B (SAT)</td><td>4808 (done)</td><td>1.73</td><td>&#8212;</td></tr>
        </table>

        <h3>SFT Training Summary</h3>
        <table>
            <tr><th>Model</th><th>Job</th><th>Epochs</th><th>Final Loss</th><th>Runtime</th></tr>
            <tr><td>DeepSeek-7B</td><td>85899 (v2)</td><td>10</td><td>0.085</td><td>~75 min</td></tr>
            <tr><td>DeepSeek-14B</td><td>85900 (v2)</td><td>10</td><td>0.067</td><td>~150 min</td></tr>
            <tr><td>Qwen3-8B</td><td>85832</td><td>5</td><td>0.168</td><td>41 min</td></tr>
            <tr><td>QwQ-32B</td><td>85834</td><td>5</td><td style="color:#b45309">0.0002 <small>(suspicious)</small></td><td>112 min</td></tr>
        </table>

        <div class="finding warn">
            <strong>QwQ-32B anomaly:</strong> SFT loss drops to 0.0002 &mdash; near-perfect memorization. This likely means QwQ has enough capacity to overfit the 6K SFT examples completely. The resulting GRPO runs (85764, 85881) are both struggling with low/negative reward. Possible fix: fewer SFT epochs (1&ndash;2), or larger/more diverse SFT data.
        </div>

        <!-- ── Section 6: Infrastructure Stats ── -->
        <h2>6. Infrastructure Overview</h2>

        <table>
            <tr><th>Metric</th><th>Count</th></tr>
            <tr><td>Total model directories</td><td>177</td></tr>
            <tr><td>Completed GRPO runs</td><td>31</td></tr>
            <tr><td>Partial / ongoing runs</td><td>58</td></tr>
            <tr><td>SFT directories (with checkpoints)</td><td>24</td></tr>
            <tr><td>Crashed runs (0 checkpoints)</td><td>64 (36% failure rate)</td></tr>
            <tr><td>Eval result JSON files</td><td>25</td></tr>
            <tr><td>Active SLURM jobs</td><td>11</td></tr>
        </table>

        <div class="finding bad">
            <strong>36% crash rate</strong> across all training attempts. Primary causes: OOM on 14B/32B models, vLLM initialization failures, NCCL timeouts on multi-node. Each crashed run wastes 10&ndash;60 min of GPU allocation before failing.
        </div>

    </div>
</div>

<!-- ═══════════════════════════════════════════════════════════════════════ -->
<!-- NOTE 0: Beta=0 Discovery                                               -->
<!-- ═══════════════════════════════════════════════════════════════════════ -->
<div class="note open" id="beta-zero">
    <div class="note-header" onclick="this.parentElement.classList.toggle('open')">
        <span class="note-chevron">&#x25B6;</span>
        <span class="note-title">The Beta=0 Discovery: Why KL Penalty Destroys GRPO Training</span>
        <span class="note-date">2026-02-21</span>
    </div>
    <div class="note-body">

        <div class="finding good">
            <strong>Key finding:</strong> Our best-ever run (85855, reward 2.66) had <code>KL=NaN</code> at every step &mdash; effectively training with <strong>zero KL penalty</strong> despite <code>beta=0.04</code>. Setting <code>beta=0</code> explicitly reproduced and exceeded this result. Any <code>beta&gt;0</code> with working KL computation causes catastrophic instability.
        </div>

        <h2>The Mystery: Why Did 85855 Succeed?</h2>

        <p>Job 85855 was our reference "successful" GRPO run: reward reached 2.66, consistency 0.84, format 0.77. We assumed its hyperparameters (<code>beta=0.04</code>, <code>LR=5e-5</code>) were the recipe. But when we launched new runs with the same settings from the SFT v2 checkpoint, they all exploded.</p>

        <table>
            <tr><th>Run</th><th>Config</th><th>KL at Step 20</th><th>Grad Norm Peak</th><th>Outcome</th></tr>
            <tr><td>85855</td><td>beta=0.04, LR=5e-5</td><td style="color:#16a34a"><strong>NaN</strong></td><td>3.7</td><td style="color:#16a34a">Reward 2.66</td></tr>
            <tr><td>85920</td><td>beta=0.01, LR=5e-5</td><td style="color:#dc2626">2,459</td><td style="color:#dc2626">1.7M</td><td style="color:#dc2626">Cancelled (unstable)</td></tr>
            <tr><td>85929</td><td>beta=0.04, LR=2e-5</td><td style="color:#dc2626">23,020</td><td style="color:#dc2626">675,840</td><td style="color:#dc2626">Cancelled (KL dominated)</td></tr>
        </table>

        <h2>Root Cause: NaN KL = No Penalty</h2>

        <p>Examining the metrics CSV from 85855 revealed the smoking gun: <strong>every single row had <code>kl=NaN</code></strong> across all 2000+ steps. When KL computes to NaN, PyTorch's loss computation effectively ignores the KL term entirely. The model was training with pure GRPO advantage signal and zero KL constraint.</p>

        <div class="finding">
            <strong>The math:</strong> GRPO loss = policy_gradient_loss + beta * KL_divergence. When KL=NaN, this becomes loss = policy_gradient_loss + 0.04 * NaN = NaN... but TRL/PyTorch handles this by masking or skipping the NaN term, so effectively <strong>loss = policy_gradient_loss</strong> alone.
        </div>

        <p>The NaN KL is likely tied to <code>fast_inference=True</code> (Unsloth vLLM engine). When TRL creates the reference model for KL computation, something in the Unsloth fast-inference path produces NaN log-probabilities. With <code>fast_inference=False</code> (14B config, TRL-managed vLLM), KL computes correctly &mdash; and immediately dominates the loss.</p>

        <h3>Proof: KL Dominates When It Works</h3>

        <p>In run 85929 (<code>beta=0.04</code>, working KL):</p>
        <pre><code>Step 20: loss = -0.013, KL = 23,020
         loss / KL = 0.013 / 23020 = 0.0000006

         KL contribution to loss: beta * KL = 0.04 * 23020 = 921
         Policy gradient contribution: ~0.01

         Ratio: KL term is 92,100x larger than the useful signal</code></pre>

        <p>The policy gradient (which carries the reward signal) is completely drowned out. The optimizer sees only "minimize KL" and ignores the reward entirely.</p>

        <h2>The Fix: beta=0</h2>

        <p>Setting <code>beta=0.0</code> explicitly removes KL penalty. Run 85934 with beta=0 from SFT v2 checkpoint:</p>

        <table>
            <tr><th>Step</th><th>Total Reward</th><th>Consistency</th><th>Format</th><th>Answer</th><th>Grad Norm</th></tr>
            <tr><td>40</td><td>0.20</td><td>-0.03</td><td>0.24</td><td>0.54</td><td>8.1</td></tr>
            <tr><td>90</td><td>0.59</td><td>+0.08</td><td>0.27</td><td>0.61</td><td>5.2</td></tr>
            <tr><td>180</td><td>1.63</td><td>+0.52</td><td>0.51</td><td>0.81</td><td>2.4</td></tr>
            <tr><td>300</td><td>1.95</td><td>+0.52</td><td>0.66</td><td>0.89</td><td>1.3</td></tr>
            <tr><td>600</td><td>2.40</td><td>+0.82</td><td>0.73</td><td>0.94</td><td>1.2</td></tr>
            <tr><td>1030</td><td style="color:#16a34a;font-weight:700">2.66</td><td style="color:#16a34a;font-weight:700">+0.84</td><td style="color:#16a34a;font-weight:700">0.77</td><td style="color:#16a34a;font-weight:700">1.00</td><td>0.8</td></tr>
            <tr><td>1260</td><td style="color:#16a34a;font-weight:700">2.70</td><td style="color:#16a34a;font-weight:700">+0.90</td><td>0.79</td><td>0.88</td><td>0.7</td></tr>
        </table>

        <div class="finding good">
            <strong>Result:</strong> Reward 2.70, consistency 0.90, format 0.79, 100% answer accuracy at peak. Grad norms stay below 10 throughout. This is the best GRPO result in the project &mdash; matching 85855's performance with a <strong>known, reproducible</strong> configuration.
        </div>

        <h2>Why Beta=0 Works Here</h2>

        <p>Standard RL wisdom says KL penalty prevents reward hacking and mode collapse. Why does removing it work for RAILS?</p>

        <ol>
            <li><strong>SFT warm-start provides the anchor.</strong> The LoRA weights are initialized from a 10-epoch SFT checkpoint that already produces reasonable format and structure. The model starts close to a good policy.</li>
            <li><strong>Multi-signal reward is self-regularizing.</strong> The 4-component reward (consistency + format + answer + length) constrains the output space. The model can't hack one signal without losing another.</li>
            <li><strong>LoRA limits capacity.</strong> With rank-16 LoRA, the model can't deviate as far from the base weights as full fine-tuning would allow. This acts as implicit regularization.</li>
            <li><strong>Clean context propagation.</strong> Invalid steps don't enter the context for subsequent verification. This makes reward hacking progressively harder (can't build on a fake foundation).</li>
        </ol>

        <h2>Implications</h2>

        <ul>
            <li><strong>For RAILS:</strong> All future GRPO configs should use <code>beta=0</code>. The 14B config (v4, job 85954) already uses this.</li>
            <li><strong>For the paper:</strong> This is an interesting finding &mdash; with sufficiently rich reward signals and SFT warm-start, KL penalty is unnecessary and harmful. Could be its own ablation study.</li>
            <li><strong>For reproducibility:</strong> The "successful" 85855 run was not reproducible because it depended on a NaN KL bug. The beta=0 configuration makes the same behavior explicit and reliable.</li>
        </ul>

        <h2>Active Runs</h2>
        <table>
            <tr><th>Job</th><th>Model</th><th>Config</th><th>Status</th></tr>
            <tr><td>85934</td><td>DeepSeek-7B</td><td>v3 (beta=0, LR=2e-5)</td><td style="color:#16a34a">Step 1260, reward 2.70</td></tr>
            <tr><td>85954</td><td>DeepSeek-14B</td><td>v4 (beta=0, LR=1e-5)</td><td>Step 30, early (stable so far)</td></tr>
        </table>

    </div>
</div>

<!-- ═══════════════════════════════════════════════════════════════════════ -->
<!-- NOTE 1: Think Template Divergence                                      -->
<!-- ═══════════════════════════════════════════════════════════════════════ -->
<div class="note open" id="think-template">
    <div class="note-header" onclick="this.parentElement.classList.toggle('open')">
        <span class="note-chevron">&#x25B6;</span>
        <span class="note-title">Think Template Divergence: DeepSeek-R1 vs Qwen3/QwQ</span>
        <span class="note-date">2026-02-21</span>
    </div>
    <div class="note-body">

        <div class="finding">
            <strong>Key finding:</strong> Models with <code>&lt;think&gt;</code> support fall into two camps. DeepSeek-R1 <strong>strips</strong> reasoning content (disposable scratchpad), while Qwen3 and QwQ <strong>preserve</strong> it (real output). This causes DeepSeek to resist learning structured <code>&lt;step&gt;</code> tags, requiring 2x SFT and 5x higher KL penalty.
        </div>

        <h2>The Two Camps</h2>

        <table>
            <tr><th></th><th>DeepSeek-R1 (Stripped)</th><th>Qwen3 / QwQ (Preserved)</th></tr>
            <tr><td><strong>Template has <code>&lt;think&gt;</code>?</strong></td><td>Yes</td><td>Yes</td></tr>
            <tr><td><strong>What happens to content?</strong></td><td style="color:#dc2626;font-weight:600">Deleted from output</td><td style="color:#16a34a;font-weight:600">Kept in output</td></tr>
            <tr><td><strong>Pretrained behavior</strong></td><td>Throwaway scratchpad</td><td>Real, structured output</td></tr>
            <tr><td><strong>Models</strong></td><td>DeepSeek-R1-7B, 14B, all distillations</td><td>Qwen3-8B, QwQ-32B, Qwen family</td></tr>
        </table>

        <h3>DeepSeek-R1: Stripping Logic</h3>
        <pre><code>{% if '&lt;/think&gt;' in content %}
  {% set content = content.split('&lt;/think&gt;')[-1] %}
{% endif %}</code></pre>
        <p>Everything between <code>&lt;think&gt;</code> and <code>&lt;/think&gt;</code> is deleted. The model learned: thinking = throwaway.</p>

        <h3>Qwen3 / QwQ: Preserving Logic</h3>
        <pre><code>&lt;|im_start|&gt;assistant
&lt;think&gt;
reasoning here          &larr; PRESERVED in output
&lt;/think&gt;

Final answer
&lt;|im_end|&gt;</code></pre>
        <p>Both reasoning and answer are rendered. The model learned: thinking = real output that matters.</p>

        <h2>Empirical Evidence</h2>

        <p>All four models trained with identical pipelines: SFT warmup (5 epochs) &rarr; GRPO with SAT reward.</p>

        <h3>Format Reward Over Training</h3>
        <table>
            <tr><th>Model</th><th>Camp</th><th>Format (start)</th><th>Format (latest)</th><th>Peak</th></tr>
            <tr><td>Qwen3-8B</td><td style="color:#16a34a">Preserved</td><td>0.45</td><td style="color:#16a34a;font-weight:700">0.93</td><td>0.95</td></tr>
            <tr><td>QwQ-32B</td><td style="color:#16a34a">Preserved</td><td>-0.83</td><td>0.56</td><td>0.59</td></tr>
            <tr style="background:#fff5f5"><td>DeepSeek-7B</td><td style="color:#dc2626">Stripped</td><td>0.03</td><td style="color:#b45309">0.72</td><td>0.77</td></tr>
            <tr style="background:#fff5f5"><td>DeepSeek-14B</td><td style="color:#dc2626">Stripped</td><td>-0.25</td><td style="color:#dc2626;font-weight:700">0.14</td><td>0.14</td></tr>
        </table>

        <div class="finding warn">
            Qwen3 starts at <strong>15x</strong> the format reward of DeepSeek-7B and reaches a <strong>30% higher</strong> plateau.
        </div>

        <h3>KL Divergence (Policy Stability)</h3>
        <table>
            <tr><th>Model</th><th>Camp</th><th>Typical KL</th><th>Peak KL</th><th>Stable?</th></tr>
            <tr><td>Qwen3-8B</td><td style="color:#16a34a">Preserved</td><td>0.4&ndash;0.5</td><td>0.5</td><td style="color:#16a34a">Yes</td></tr>
            <tr><td>QwQ-32B</td><td style="color:#16a34a">Preserved</td><td>0.02&ndash;0.11</td><td>5.6</td><td style="color:#16a34a">Yes (after step 80)</td></tr>
            <tr style="background:#fff5f5"><td>DeepSeek-7B</td><td style="color:#dc2626">Stripped</td><td style="color:#dc2626;font-weight:600">15&ndash;30</td><td style="color:#dc2626;font-weight:600">1,385</td><td style="color:#dc2626;font-weight:600">No &mdash; wild oscillations</td></tr>
            <tr style="background:#fff5f5"><td>DeepSeek-14B</td><td style="color:#dc2626">Stripped</td><td>0.4&ndash;0.7</td><td style="color:#dc2626">51.6</td><td style="color:#dc2626">No &mdash; early instability</td></tr>
        </table>

        <div class="finding bad">
            DeepSeek-7B's KL reaches <strong>1,385</strong> at step 20 &mdash; complete policy divergence. It oscillates between 8 and 33 for thousands of steps, causing garbled outputs with broken Unicode and Chinese characters.
        </div>

        <h3>Output Quality: Qwen3 vs DeepSeek (Step 4000)</h3>

        <p><strong>Qwen3-8B</strong> &mdash; clean, structured, parseable:</p>
        <div class="sample"><span class="good-tag">&lt;think&gt;</span>
Let's formalize the premises and analyze the conclusion.

<span class="good-tag">&lt;step n="1" type="premise"&gt;</span>
<span class="good-tag">&lt;text&gt;</span>If John wakes up early, then he will meditate.<span class="good-tag">&lt;/text&gt;</span>
<span class="good-tag">&lt;logic&gt;</span>meditate :- wakes_up_early.<span class="good-tag">&lt;/logic&gt;</span>
<span class="good-tag">&lt;/step&gt;</span>
...
<span class="good-tag">&lt;/think&gt;</span>

No</div>

        <p><strong>DeepSeek-7B</strong> &mdash; occasional degeneration:</p>
        <div class="sample">&lt;step n="4" type="derived" from="2"&gt;
&lt;text&gt;From premise 2: If he goes abroad, it implies he doesn't
having a nationality.<span class="garbled">&lt;/.text&gt;</span>
Hold that step, it seem <span class="garbled">problematic&#x5947;&#x7EB3;</span>. Maybe let me think another way.</div>

        <p><strong>DeepSeek-7B</strong> &mdash; worst case (1/10 at step 4000):</p>
        <div class="sample">&lt;step n="1" type="premise"&gt;
&lt;text&gt;At least one of the following is true...&lt;/text&gt;
&lt;logic&gt;<span class="garbled">(producing(&#x699D;)) StudyExam(Michael). \&#x2228;. Heading movies(Mayrlia).</span>&lt;/logic&gt;</div>

        <h2>Root Cause Analysis</h2>

        <h3>1. Pretrained Weight Bias</h3>
        <p>Even after patching the template to stop stripping, DeepSeek-R1's pretrained weights carry the bias: "content inside <code>&lt;think&gt;</code> is throwaway." This is distributed across billions of parameters &mdash; not just a template string, but a learned behavior about output quality inside vs. outside <code>&lt;think&gt;</code>.</p>

        <h3>2. Conflicting Semantics</h3>
        <table>
            <tr><th>What RAILS wants</th><th>What DeepSeek-R1 learned</th></tr>
            <tr><td>Structured <code>&lt;step&gt;</code> tags</td><td>Free-form monologue</td></tr>
            <tr><td>Parseable <code>&lt;logic&gt;</code> fields</td><td>Natural language brainstorming</td></tr>
            <tr><td>Consistent format throughout</td><td>Quality degrades as "thinking" continues</td></tr>
        </table>
        <p>This creates a tug-of-war during GRPO: reward pushes toward structure, pretrained distribution pulls toward freeform. The result is KL instability &mdash; the model oscillates between modes.</p>

        <h3>3. GRPO Beta Sensitivity</h3>
        <p>With <code>beta=0.01</code>, the KL penalty is too weak to prevent DeepSeek from diverging. Qwen handles <code>beta=0.01</code> fine because its reference policy already produces good format. DeepSeek needs <code>beta=0.05&ndash;0.10</code> to stay anchored.</p>

        <h2>Generalized Insight</h2>

        <div class="finding good">
            <strong>Template architecture matters for verification training.</strong> Before choosing a base model for step-level verification, check how its chat template handles reasoning tokens:
        </div>

        <ul>
            <li><strong>Preserved reasoning</strong> (Qwen3, QwQ, and other ChatML-family models): Good candidates. Structured formats inside <code>&lt;think&gt;</code> align with pretrained behavior.</li>
            <li><strong>Stripped reasoning</strong> (DeepSeek-R1, "hidden CoT" models): Requires mitigation &mdash; stronger SFT (2x epochs), higher KL penalty (5x beta), or a different tag to avoid conflict.</li>
        </ul>

        <p>This extends beyond logic: any system needing structured intermediate reasoning (math proofs, code traces, planning states) will face this compatibility question.</p>

        <h2>Remediation</h2>
        <table>
            <tr><th>Change</th><th>Config</th><th>Rationale</th></tr>
            <tr><td>SFT epochs: 5 &rarr; 10</td><td><code>deepseek_sft_think_v2.yaml</code></td><td>Stronger override of scratchpad bias</td></tr>
            <tr><td>SFT data: 4,364 &rarr; 6,033</td><td>Rebuilt <code>think_all.jsonl</code></td><td>More diverse format examples</td></tr>
            <tr><td>SFT LR: 1.5e-4 &rarr; 1.0e-4</td><td>Same config</td><td>Avoid overfitting on longer training</td></tr>
            <tr><td>GRPO beta: 0.04 &rarr; <strong>0.0</strong></td><td><code>deepseek_sat_think_from_sft_v3.yaml</code></td><td>KL penalty is harmful with SFT warm-start (see <a href="#beta-zero">beta=0 note</a>)</td></tr>
        </table>

        <p><strong>Result:</strong> SFT v2 + beta=0 GRPO (job 85934) achieved format reward <strong>0.79</strong>, consistency <strong>0.90</strong>, total reward <strong>2.70</strong>. KL stays at 0 (no penalty), grad norms &lt;10. Garbled outputs eliminated.</p>

    </div>
</div>

<!-- ═══════════════════════════════════════════════════════════════════════ -->
<!-- NOTE 2: NLI vs SAT                                                     -->
<!-- ═══════════════════════════════════════════════════════════════════════ -->
<div class="note" id="nli-vs-sat">
    <div class="note-header" onclick="this.parentElement.classList.toggle('open')">
        <span class="note-chevron">&#x25B6;</span>
        <span class="note-title">NLI vs SAT: Why Formal Verification Beats Soft Approximation</span>
        <span class="note-date">2026-02-20</span>
    </div>
    <div class="note-body">

        <div class="finding bad">
            <strong>Core result:</strong> DeBERTa-large-MNLI (NLI) false-accepts <strong>3 of 5</strong> logically invalid reasoning chains. SAT correctly rejects all 5. NLI gives <strong>+0.84</strong> to a reversed implication.
        </div>

        <h2>Results</h2>
        <table>
            <tr><th>#</th><th>Example</th><th>NLI Score</th><th>SAT Score</th><th>NLI Verdict</th></tr>
            <tr><td>1</td><td>Reversed Implication (P&rarr;Q, Q &there4; P)</td><td style="color:#dc2626;font-weight:600">+0.840</td><td style="color:#16a34a">-1.000</td><td style="color:#dc2626">FALSE ACCEPT</td></tr>
            <tr><td>2</td><td>Inverse &ne; Contrapositive (~P&rarr;~Q from P&rarr;Q)</td><td style="color:#dc2626;font-weight:600">+0.878</td><td style="color:#16a34a">-1.000</td><td style="color:#dc2626">FALSE ACCEPT</td></tr>
            <tr><td>3</td><td>Denying the Antecedent (~P &there4; ~Q)</td><td>-0.057</td><td>+0.000</td><td style="color:#16a34a">Correct reject</td></tr>
            <tr><td>4</td><td>Logical Babble (no consistency reward, 7 steps)</td><td style="color:#dc2626;font-weight:600">+0.021</td><td style="color:#16a34a">-1.000</td><td style="color:#dc2626">FALSE ACCEPT</td></tr>
            <tr><td>5</td><td>Hidden Negation (valid NL, bad formal)</td><td>-0.923</td><td>-1.000</td><td style="color:#16a34a">Correct reject</td></tr>
        </table>

        <h2>The Three Killer Examples</h2>

        <h3>Example 1: Affirming the Consequent (NLI: +0.84, SAT: -1.0)</h3>
        <p>Source: Qwen3-8B, GRPO step 1600</p>
        <div class="sample">Premise:  go_to_party :- gold_medal.     "If gold medal, then party"
Given:    go_to_party.                    "He's going to a party"
Derived:  gold_medal :- go_to_party.      "Since party, he has gold medal"  <span class="bad-tag">&larr; INVALID</span></div>
        <p>The model reverses the implication. DeBERTa gives 88.5% entailment because "gold medal" and "party" co-occur. It treats "A implies B" and "B implies A" as near-paraphrases.</p>

        <h3>Example 2: Inverse Fallacy (NLI: +0.88, SAT: -1.0)</h3>
        <p>Source: Qwen3-8B, GRPO step 2400</p>
        <div class="sample">Premise:  weak :- skips.           "If skips breakfast &rarr; feels weak"
Given:    ~skips.                  "Doesn't skip breakfast"
Derived:  ~weak :- ~skips.         "Doesn't skip &rarr; doesn't feel weak"  <span class="bad-tag">&larr; INVALID (inverse, not contrapositive)</span></div>
        <p>The correct contrapositive of <code>skips&rarr;weak</code> is <code>~weak&rarr;~skips</code>, but the model writes <code>~skips&rarr;~weak</code> (the inverse). NLI gives 85.4% because the text sounds perfectly natural. Humans make this same error.</p>

        <h3>Example 4: Logical Babble (NLI: +0.02, SAT: -1.0)</h3>
        <p>Source: GRPO baseline without consistency reward, step 2900</p>
        <p>Without consistency reward, the model produces 7 steps citing real rule names (Destructive Dilemma, Disjunctive Syllogism, Hypothetical Syllogism) &mdash; creating an <em>illusion</em> of rigorous reasoning. Every derived step fails SAT verification.</p>

        <h2>Why NLI Fails</h2>
        <ol>
            <li><strong>Lexical overlap heuristic:</strong> NLI models rely on word overlap. "If gold medal then party" and "if party then gold medal" share the same words &rarr; high score.</li>
            <li><strong>Human annotation bias:</strong> NLI training data (SNLI, MultiNLI) annotated by crowd workers who make the same fallacies. The model inherits these biases.</li>
            <li><strong>No structural awareness:</strong> NLI operates on surface text. It cannot distinguish P&rarr;Q from Q&rarr;P, or valid from invalid rule application.</li>
        </ol>

        <h2>The Baseline Gap (No Consistency Reward)</h2>

        <div class="finding warn">
            Without consistency reward: <strong>80% answer accuracy</strong> but <strong>-90% consistency score</strong>. Models learn correct answers through invalid reasoning. NLI would accept most of these chains.
        </div>

        <h2>Paper Framing</h2>
        <p>Three-part argument:</p>
        <ol>
            <li>Models learn right answers through invalid reasoning (80% accuracy, -90% consistency)</li>
            <li>NLI cannot catch the errors (60% false acceptance, +0.84 on reversed implications)</li>
            <li>Formal verification catches every error (100% true rejection rate)</li>
        </ol>

        <h2>Supporting Literature</h2>
        <ul>
            <li><strong>HANS</strong> (McCoy et al., 2019): NLI models rely on lexical overlap, not logical structure</li>
            <li><strong>LogicNLI</strong> (Tian et al., 2021): NLI accuracy drops to ~50% (chance) on propositional logic</li>
            <li><strong>Turpin et al.</strong> (NeurIPS 2023): CoT explanations are systematically unfaithful</li>
            <li><strong>Anthropic</strong> (2025): Claude is only 41% faithful in CoT, DeepSeek-R1 only 19%</li>
            <li><strong>LINC</strong> (Olausson et al., 2023): Formal logic (Prolog) gives 26% higher accuracy than CoT on ProofWriter</li>
        </ul>

    </div>
</div>

<!-- ═══════════════════════════════════════════════════════════════════════ -->
<!-- NOTE 3: Real-World Motivation                                          -->
<!-- ═══════════════════════════════════════════════════════════════════════ -->
<div class="note" id="real-world">
    <div class="note-header" onclick="this.parentElement.classList.toggle('open')">
        <span class="note-chevron">&#x25B6;</span>
        <span class="note-title">Connecting RAILS to Science and Law</span>
        <span class="note-date">2026-02-20</span>
    </div>
    <div class="note-body">

        <div class="finding">
            In domains requiring rigorous reasoning &mdash; science, medicine, law &mdash; plausible-but-invalid chains of inference have real consequences. NLI-style checking fails on exactly the fallacies that matter most.
        </div>

        <h2>Why These Domains Need Formal Verification</h2>

        <table>
            <tr><th>Domain</th><th>What goes wrong with NLI</th><th>What formal verification catches</th></tr>
            <tr><td><strong>Science</strong></td><td>"If temp increases, pressure increases" scored same as reverse by NLI</td><td>SAT catches P&rarr;Q &ne; Q&rarr;P</td></tr>
            <tr><td><strong>Medicine</strong></td><td>"If symptom A then disease B" confused with "If disease B then symptom A"</td><td>Formal verification preserves conditional direction</td></tr>
            <tr><td><strong>Law</strong></td><td>"If conditions X,Y,Z met then statute applies" &mdash; NLI can't verify all conditions checked</td><td>SAT/Prolog verifies each condition is established</td></tr>
            <tr><td><strong>Math</strong></td><td>"Since A implies B, and B is true, therefore A" sounds plausible to NLI</td><td>SAT rejects affirming the consequent</td></tr>
        </table>

        <h2>Scientific Reasoning Benchmarks</h2>

        <h3>Tier 1: Directly Amenable to Formal Verification</h3>
        <table>
            <tr><th>Benchmark</th><th>Domain</th><th>Size</th><th>Verification</th><th>Reference</th></tr>
            <tr><td><strong>PhysReason</strong></td><td>Physics</td><td>1,200 (avg 8.1 steps)</td><td>SymPy</td><td>ACL 2025</td></tr>
            <tr><td><strong>EngTrace</strong></td><td>Engineering</td><td>1,350 cases</td><td>Symbolic templates</td><td>arXiv:2511.01650</td></tr>
            <tr><td><strong>FormalMATH</strong></td><td>Math (Lean4)</td><td>5,560 proofs</td><td>Lean4 proof checker</td><td>arXiv:2505.02735</td></tr>
            <tr><td><strong>SATBench</strong></td><td>Logic (SAT)</td><td>2,100 puzzles</td><td>SAT solver</td><td>EMNLP 2025</td></tr>
        </table>

        <h3>Tier 2: Process-Level Evaluation</h3>
        <table>
            <tr><th>Benchmark</th><th>Domain</th><th>Key Finding</th></tr>
            <tr><td><strong>PRM800K</strong></td><td>Math</td><td>Process supervision &gt;&gt; outcome supervision (78% vs 72% on MATH). Foundational justification for step-level rewards.</td></tr>
            <tr><td><strong>PRMBench</strong></td><td>Math + STEM</td><td>6,216 problems / 83K step labels. 9 error types.</td></tr>
            <tr><td><strong>RFEval</strong></td><td>Multi-domain</td><td style="color:#dc2626;font-weight:600">49.7% of LLM outputs are unfaithful. Covers code, math, logic, legal.</td></tr>
            <tr><td><strong>MR-Ben</strong></td><td>Multi-domain</td><td>Meta-reasoning: can models detect errors in reasoning steps?</td></tr>
        </table>

        <div class="finding bad">
            <strong>RFEval (Feb 2026)</strong> finds that <strong>49.7% of LLM outputs are unfaithful</strong> &mdash; accuracy is not a reliable proxy for reasoning faithfulness. This directly validates our thesis.
        </div>

        <h3>Most Related System: LogicReward</h3>
        <p><strong>LogicReward</strong> (arXiv:2512.18196, Dec 2025): theorem provers for step-level verification during RL. 8B model surpasses GPT-4o by 11.6%.</p>
        <table>
            <tr><th></th><th>LogicReward</th><th>RAILS</th></tr>
            <tr><td>Focus</td><td>NLI tasks</td><td>Broader: propositional, FOL, non-monotonic</td></tr>
            <tr><td>Verifier</td><td>Generic theorem prover</td><td>Domain-specific (SAT, ASP, NSFR) + clean context</td></tr>
            <tr><td>Answer gating</td><td>No</td><td>Yes (no credit for right answer + wrong reasoning)</td></tr>
        </table>

        <h2>Legal Reasoning Benchmarks</h2>

        <h3>Tier 1: Perfect Fit for Formal Verification</h3>
        <table>
            <tr><th>Benchmark</th><th>Size</th><th>Why It Fits RAILS</th></tr>
            <tr><td><strong>SARA</strong></td><td>376 cases</td><td>Tax statutes as Horn clauses. A hand-constructed Prolog system fully solves it. NLI fails.</td></tr>
            <tr><td><strong>Catala</strong></td><td>Tax law sections</td><td>Uses <strong>prioritized default logic</strong> &mdash; same as our ASP/Clingo backend.</td></tr>
            <tr><td><strong>LLMs + Prolog</strong></td><td>SARA-based</td><td>Neuro-symbolic: LLM generates Prolog, solver verifies. Validates RAILS paradigm.</td></tr>
        </table>

        <h3>Example: Legal Reasoning Failure</h3>
        <div class="sample">Statute: "If gross income &gt; $10K AND not dependent &rarr; must file return"

Facts:   Alice filed a return. Alice has income $15K.
Model:   "Alice is not claimed as a dependent."  <span class="bad-tag">&larr; INVALID (affirming the consequent)</span>

(A &and; B &rarr; C) &and; C &and; A does NOT entail B
NLI: "sounds right" (+high score)
SAT: rejected (invalid inference)</div>

        <h3>Tier 2: Step-Level Legal Reasoning</h3>
        <table>
            <tr><th>Benchmark</th><th>Size</th><th>Key Feature</th></tr>
            <tr><td><strong>MSLR</strong></td><td>1,400 cases / 60K step annotations</td><td>Most granular step-wise legal reasoning. IRAC framework.</td></tr>
            <tr><td><strong>COLIEE</strong></td><td>7,350+ files</td><td>Japanese Bar exam. Annual competition since 2014.</td></tr>
            <tr><td><strong>MASLegalBench</strong></td><td>950 MCQs</td><td>GDPR reasoning decomposed into 4 verifiable steps.</td></tr>
            <tr><td><strong>LAR-ECHR</strong></td><td>403 instances</td><td>Argument chain validity. GPT-4o only 75.8%.</td></tr>
        </table>

        <h2>Paper Framing</h2>
        <p>The story in 4 beats:</p>
        <ol>
            <li><strong>Models reach correct answers through invalid reasoning</strong> (80% accuracy, -90% consistency)</li>
            <li><strong>Soft NLI misses the errors</strong> (+0.84 to reversed implications)</li>
            <li><strong>This matters in high-stakes domains</strong>: confusing P&rarr;Q with Q&rarr;P means confusing correlation with causation in science, misapplying statutes in law, base rate neglect in medicine</li>
            <li><strong>RAILS solves this</strong> with symbolic verification rewards producing provably valid reasoning</li>
        </ol>

    </div>
</div>

<div class="footer">
    Updated February 22, 2026 &middot;
    RAILS (Reasoning with Automatically Integrated Logical Supervision) &middot;
    <a href="index.html">Project Hub</a>
</div>

<script>
// Allow clicking TOC links to open the corresponding note
document.querySelectorAll('.toc-link').forEach(link => {
    link.addEventListener('click', e => {
        e.preventDefault();
        const id = link.getAttribute('href').slice(1);
        const note = document.getElementById(id);
        if (note && !note.classList.contains('open')) {
            note.classList.add('open');
        }
        note.scrollIntoView({ behavior: 'smooth', block: 'start' });
    });
});
</script>

</body>
</html>
