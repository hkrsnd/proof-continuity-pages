<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RAILS â€” Qualitative Analysis</title>
<style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
    background: #f8f9fa;
    color: #1a1a2e;
    line-height: 1.6;
    padding: 2rem;
    max-width: 1100px;
    margin: 0 auto;
}
h1 { font-size: 1.8rem; margin-bottom: 0.3rem; color: #16213e; }
h2 { font-size: 1.3rem; color: #16213e; margin-bottom: 0.8rem; }
h3 { font-size: 1.05rem; color: #374151; margin-bottom: 0.5rem; }
.subtitle { color: #666; font-size: 0.95rem; margin-bottom: 2rem; }

/* Summary */
.summary { background: white; border-radius: 12px; padding: 1.5rem; margin-bottom: 1.5rem; box-shadow: 0 1px 3px rgba(0,0,0,0.08); }
.stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(130px, 1fr)); gap: 0.8rem; margin-bottom: 1rem; }
.stat { background: #f0f4ff; border-radius: 8px; padding: 0.8rem; text-align: center; }
.stat-value { font-size: 1.5rem; font-weight: 700; color: #2563eb; }
.stat-label { font-size: 0.75rem; color: #666; margin-top: 0.2rem; }
.stat-green .stat-value { color: #16a34a; }
.stat-amber .stat-value { color: #d97706; }

/* Badges */
.badge { display: inline-block; padding: 0.2rem 0.6rem; border-radius: 20px; font-size: 0.75rem; font-weight: 600; }
.badge-yes { background: #dcfce7; color: #166534; }
.badge-no { background: #fef3c7; color: #92400e; }
.badge-wrong { background: #fee2e2; color: #991b1b; }
.badge-correct { background: #dcfce7; color: #166534; }
.badge-theory { background: #e0e7ff; color: #3730a3; margin-left: 0.4rem; }
.badge-model { background: #f3e8ff; color: #6b21a8; }

/* Example container */
.example { background: white; border-radius: 12px; padding: 1.5rem; margin-bottom: 2rem; box-shadow: 0 1px 3px rgba(0,0,0,0.08); }
.example-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem; padding-bottom: 0.8rem; border-bottom: 1px solid #eee; flex-wrap: wrap; gap: 0.5rem; }

/* Context and question */
.context { background: #fafafa; border-left: 3px solid #94a3b8; padding: 0.8rem 1rem; margin-bottom: 0.8rem; border-radius: 0 6px 6px 0; font-size: 0.88rem; }
.context-label { font-size: 0.7rem; text-transform: uppercase; letter-spacing: 0.05em; color: #94a3b8; font-weight: 600; margin-bottom: 0.3rem; }
.question { background: #fffbeb; border-left: 3px solid #f59e0b; padding: 0.8rem 1rem; margin-bottom: 0.8rem; border-radius: 0 6px 6px 0; font-size: 0.9rem; font-weight: 500; }
.ground-truth { font-size: 0.82rem; color: #16a34a; font-weight: 600; margin-bottom: 1rem; }

/* Side-by-side comparison */
.comparison { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin-top: 1rem; }
@media (max-width: 768px) { .comparison { grid-template-columns: 1fr; } }

.trace { border-radius: 8px; padding: 1rem; font-size: 0.82rem; overflow-x: auto; }
.trace-base { background: #fef2f2; border: 1px solid #fecaca; }
.trace-sat { background: #f0fdf4; border: 1px solid #bbf7d0; }
.trace-answer-only { background: #fffbeb; border: 1px solid #fde68a; }
.trace-diff { background: #f5f3ff; border: 1px solid #ddd6fe; }

.trace-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 0.6rem; padding-bottom: 0.4rem; border-bottom: 1px solid rgba(0,0,0,0.08); }
.trace-title { font-weight: 700; font-size: 0.85rem; }
.trace-base .trace-title { color: #dc2626; }
.trace-sat .trace-title { color: #16a34a; }
.trace-answer-only .trace-title { color: #d97706; }
.trace-diff .trace-title { color: #7c3aed; }

.trace-content { font-family: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace; font-size: 0.78rem; line-height: 1.5; white-space: pre-wrap; word-break: break-word; color: #374151; }
.truncated { color: #9ca3af; font-style: italic; }

/* Observation callout */
.observation { background: #eff6ff; border-left: 4px solid #3b82f6; padding: 0.8rem 1rem; margin-top: 1rem; border-radius: 0 8px 8px 0; font-size: 0.85rem; }
.observation strong { color: #1e40af; }

/* Findings */
.finding { padding: 0.8rem 1rem; border-radius: 8px; margin-bottom: 0.6rem; font-size: 0.88rem; }
.finding-positive { background: #f0fdf4; border-left: 4px solid #22c55e; }
.finding-insight { background: #eff6ff; border-left: 4px solid #3b82f6; }
.finding-caution { background: #fffbeb; border-left: 4px solid #f59e0b; }

/* Card */
.card { background: white; border-radius: 12px; padding: 1.5rem; margin-bottom: 1.5rem; box-shadow: 0 1px 3px rgba(0,0,0,0.08); }

/* Table */
table { width: 100%; border-collapse: collapse; font-size: 0.82rem; margin-top: 0.8rem; }
th { background: #f0f4ff; color: #1e40af; font-weight: 600; font-size: 0.72rem; text-transform: uppercase; letter-spacing: 0.04em; padding: 0.5rem; text-align: center; border-bottom: 2px solid #dbeafe; }
th:first-child { text-align: left; }
td { padding: 0.5rem; text-align: center; border-bottom: 1px solid #f0f0f0; }
td:first-child { text-align: left; }

/* Nav link */
.back-link { display: inline-block; margin-bottom: 1rem; color: #2563eb; text-decoration: none; font-size: 0.85rem; font-weight: 500; }
.back-link:hover { text-decoration: underline; }

/* Footer */
.footer { text-align: center; color: #94a3b8; font-size: 0.8rem; margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e5e7eb; }

/* Section label */
.section-label { font-size: 0.7rem; text-transform: uppercase; letter-spacing: 0.05em; color: #94a3b8; font-weight: 600; margin-bottom: 0.6rem; }

/* Responsive */
@media (max-width: 768px) { body { padding: 1rem; } }
</style>
</head>
<body>
<a href="index.html" class="back-link">&larr; Back to Hub</a>
<h1>RAILS &mdash; Qualitative Analysis</h1>
<p class="subtitle">Side-by-side comparison of reasoning traces: Base vs RAILS-trained models &middot; Same question, different training conditions</p>

<!-- Overview -->
<div class="summary">
    <h2>How Proof-Continuity Reward Changes Reasoning</h2>
    <p style="font-size:0.88rem; color:#555; margin-bottom:1rem;">
        Each example below shows the <strong>same question</strong> answered by the <strong>same model</strong> under different training conditions.
        This reveals how the consistency reward shapes intermediate reasoning steps &mdash; not just final answers.
    </p>
    <div class="stats">
        <div class="stat"><div class="stat-value">16</div><div class="stat-label">Paired Examples</div></div>
        <div class="stat stat-green"><div class="stat-value">11</div><div class="stat-label">Base Wrong, SAT Correct</div></div>
        <div class="stat stat-amber"><div class="stat-value">4</div><div class="stat-label">Models Compared</div></div>
        <div class="stat"><div class="stat-value">4</div><div class="stat-label">Logic Theories</div></div>
    </div>
</div>

<!-- Example 1: DeepSeek Modus Tollens -->
<div class="example">
    <div class="example-header">
        <h3>Example 1: Truncation vs Termination</h3>
        <div>
            <span class="badge badge-model">DeepSeek-R1 (7B)</span>
            <span class="badge badge-theory">Modus Tollens</span>
        </div>
    </div>

    <div class="context">
        <div class="context-label">Context (Premises)</div>
        Liam had finished his work early for the day, which meant that he would typically have ordered pizza for dinner. However, on this particular day, he decided against ordering pizza and opted for something else.
    </div>
    <div class="question">Does this imply that Liam didn't finish his work early?</div>
    <div class="ground-truth">Ground Truth: Yes (by Modus Tollens: A&rarr;B, &not;B &vdash; &not;A)</div>

    <div class="comparison">
        <div class="trace trace-base">
            <div class="trace-header">
                <span class="trace-title">Base Model</span>
                <span class="badge badge-wrong">WRONG &mdash; unknown</span>
            </div>
            <div class="trace-content">Okay, so I need to figure out if the conclusion that "Liam didn't finish his work early" logically follows from the given premises.

First, I'll break down the information. The first part says that finishing work early leads to ordering pizza. That sounds like a conditional statement: If he finished early, then he orders pizza. In FOL:
finishesEarly(Liam) &rarr; ordersPizza(Liam).

The second part is that he didn't order pizza:
&not;ordersPizza(Liam) &and; choosesElse(Liam).

Looking at the rules, I remember that Modus Tollens is the rule that applies here. Modus Tollens says that if we have A &rarr; B and we know that &not;B, then we can con<span class="truncated">[TRUNCATED &mdash; token limit reached]</span></div>
        </div>

        <div class="trace trace-sat">
            <div class="trace-header">
                <span class="trace-title">SAT (RAILS)</span>
                <span class="badge badge-correct">CORRECT &mdash; yes</span>
            </div>
            <div class="trace-content">Alright, let's break down the problem step by step.

First, I need to identify the premises. The first premise states that if Liam finished his work early, he would typically order pizza:
EarlyFinish &rarr; OrdersPizza.

The second premise tells us that Liam decided against ordering pizza:
&not;OrdersPizza.

Now, I can use the contrapositive of the first statement:
&not;OrdersPizza &rarr; &not;EarlyFinish.

Applying the contrapositive, since we know &not;OrdersPizza is true, we can conclude that &not;EarlyFinish is true.

Therefore, the conclusion is that Liam didn't finish his work early.

Answer: Yes.</div>
        </div>
    </div>

    <div class="observation">
        <strong>Key insight:</strong> The base model <em>identifies</em> Modus Tollens correctly but produces meandering prose that gets <strong>cut off</strong> before reaching a conclusion.
        The SAT model produces concise, directed reasoning: premises &rarr; rule (contrapositive) &rarr; application &rarr; conclusion. <strong>No wandering, no truncation.</strong>
        Answer-only training produces nearly identical output to base &mdash; the consistency signal is what drives structural improvement.
    </div>
</div>

<!-- Example 2: DeepSeek Hypothetical Syllogism (Invalid) -->
<div class="example">
    <div class="example-header">
        <h3>Example 2: Metacognitive Reasoning About Invalid Inferences</h3>
        <div>
            <span class="badge badge-model">DeepSeek-R1 (7B)</span>
            <span class="badge badge-theory">Hypothetical Syllogism</span>
        </div>
    </div>

    <div class="context">
        <div class="context-label">Context (Premises)</div>
        If Jim cleans his room, he will receive a reward. And if he receives a reward, he will use it to buy a new toy. Jim decided to tidy up his room, hoping to earn a reward.
    </div>
    <div class="question">If Jim didn't clean his room, does this entail that he won't buy a new toy?</div>
    <div class="ground-truth">Ground Truth: No (denying the antecedent is a fallacy)</div>

    <div class="comparison">
        <div class="trace trace-base">
            <div class="trace-header">
                <span class="trace-title">Base Model</span>
                <span class="badge badge-wrong">WRONG &mdash; unknown</span>
            </div>
            <div class="trace-content">Okay, so I'm trying to figure out whether Jim not cleaning his room means he won't buy a new toy.

The first is cleaning_room(jim) &rarr; reward(jim).
The second is reward(jim) &rarr; buys_new_toy(jim).

I need to see if &not;cleaning_room(jim) leads to &not;buys_new_toy(jim).

Looking at the given premises, I have two implications. The first is cleaning_room(jim) &rarr;<span class="truncated">[TRUNCATED &mdash; token limit reached]</span></div>
        </div>

        <div class="trace trace-sat">
            <div class="trace-header">
                <span class="trace-title">SAT (RAILS)</span>
                <span class="badge badge-correct">CORRECT &mdash; no</span>
            </div>
            <div class="trace-content">The premises are that if Jim cleans his room, he'll get a reward, and if he gets a reward, he'll buy a new toy.

First, I translate the premises:
cleanRoom(jim) &rarr; reward(jim)
reward(jim) &rarr; buyToy(jim)

Using Hypothetical Syllogism to combine the two premises:
cleanRoom(jim) &rarr; buyToy(jim)

However, this doesn't help with the negation of cleanRoom(jim).

Since there's no valid inference from the premises that connects &not;cleanRoom(jim) to &not;buyToy(jim), I conclude that the premises don't support the conclusion.

Answer: No.</div>
        </div>
    </div>

    <div class="observation">
        <strong>Key insight:</strong> The SAT model doesn't just follow rules &mdash; it reasons about <strong>when rules don't apply</strong>.
        It chains the implications via Hypothetical Syllogism, then correctly identifies that the negated antecedent cannot yield a conclusion.
        This is <strong>metacognitive reasoning about inference validity</strong>, which prevents the "denying the antecedent" fallacy.
        The Diff-Anneal model, by contrast, applies Modus Ponens in the <em>wrong direction</em> (assuming Jim cleaned when the question says he didn't).
    </div>
</div>

<!-- Example 3: DeepSeek Constructive Dilemma -->
<div class="example">
    <div class="example-header">
        <h3>Example 3: Natural Language Comprehension + Logic</h3>
        <div>
            <span class="badge badge-model">DeepSeek-R1 (7B)</span>
            <span class="badge badge-theory">Constructive Dilemma</span>
        </div>
    </div>

    <div class="context">
        <div class="context-label">Context (Premises)</div>
        Harry had two options for his day off: going to the park or going to the beach. If he decided to go to the park, it meant that he would have a lovely picnic with his family. On the other hand, if he chose to go to the beach, he would be able to swim in the ocean.
    </div>
    <div class="question">Can we say at least one of the following must always be true? (a) he will have a picnic with his family and (b) he will swim in the ocean</div>
    <div class="ground-truth">Ground Truth: Yes (by Constructive Dilemma)</div>

    <div class="comparison">
        <div class="trace trace-base">
            <div class="trace-header">
                <span class="trace-title">Base Model</span>
                <span class="badge badge-wrong">WRONG &mdash; no</span>
            </div>
            <div class="trace-content">If he goes to the park, then picnic. If he goes to the beach, then swim. Since he must go to one of them, either picnic or swim must happen.

But the question is about having both. Since he can't have both at the same time, we can't say both are true.

So, we can't conclude that both a and b are true.

Therefore, the answer is no.</div>
        </div>

        <div class="trace trace-sat">
            <div class="trace-header">
                <span class="trace-title">SAT (RAILS)</span>
                <span class="badge badge-correct">CORRECT &mdash; yes</span>
            </div>
            <div class="trace-content">I'll represent the premises in FOL:
park(h) &rarr; picnic(h)
beach(h) &rarr; swim(h)
park(h) &or; beach(h)

Using the premises, I can apply disjunctive syllogism. If Harry goes to the park, then (a) is true. If he goes to the beach, then (b) is true.

Since he must go to one of the two, at least one of (a) or (b) must be true.

Therefore, the conclusion is that at least one of the statements must be true.

Answer: Yes.</div>
        </div>
    </div>

    <div class="observation">
        <strong>Key insight:</strong> Both models perform the <em>same logical derivation</em> correctly (either picnic or swim must happen).
        But the base model <strong>misreads "at least one" as "both"</strong>, leading to the wrong answer.
        The SAT model correctly interprets the question. The consistency reward may help maintain alignment between logical conclusions and natural language interpretation.
    </div>
</div>

<!-- Example 4: Mistral Self-Correction -->
<div class="example">
    <div class="example-header">
        <h3>Example 4: Self-Correction Through Additional Steps</h3>
        <div>
            <span class="badge badge-model">Mistral (7B)</span>
            <span class="badge badge-theory">Constructive Dilemma</span>
        </div>
    </div>

    <div class="context">
        <div class="context-label">Context (Premises)</div>
        If Ava chooses to go to the hotel, she will be rewarded with a delightful ice-cream. On the other hand, if she decides to go to the movies, she will be able to indulge in some delicious popcorn. We know she will go to one of these places.
    </div>
    <div class="question">Can we say at least one of the following must always be true? (a) Ava will have ice-cream and (b) she will have popcorn</div>
    <div class="ground-truth">Ground Truth: Yes</div>

    <div class="comparison">
        <div class="trace trace-base">
            <div class="trace-header">
                <span class="trace-title">Base Model</span>
                <span class="badge badge-wrong">WRONG &mdash; unknown</span>
            </div>
            <div class="trace-content">Step 1: iceCream(Ava) :- wentToHotel(Ava).
Step 2: popcorn(Ava) :- wentToMovies(Ava).
Step 3: wentToHotel(Ava) &or; wentToMovies(Ava).

Step 4: ...by Constructive Dilemma, we cannot conclude that at least one of having an ice-cream and having popcorn must always be true. It is possible that only one of the options is true...
(Logic) &perp;</div>
        </div>

        <div class="trace trace-sat">
            <div class="trace-header">
                <span class="trace-title">SAT (RAILS)</span>
                <span class="badge badge-correct">CORRECT &mdash; yes</span>
            </div>
            <div class="trace-content">Step 1: iceCream(Ava) :- wentToHotel(Ava).
Step 2: popcorn(Ava) :- wentToMovies(Ava).
Step 3: wentToHotel(Ava) &or; wentToMovies(Ava).

Step 4: ...by Constructive Dilemma, we cannot conclude that Ava will have <em>both</em> an ice-cream and popcorn, as it depends on which action she chooses.
(Logic) &perp;

<strong style="color:#16a34a;">Step 5: However, we can say that at least one of the following must be true: Ava will have an ice-cream or she will have popcorn.
(Logic) iceCream(Ava) &or; popcorn(Ava).</strong>

Answer: Yes.</div>
        </div>
    </div>

    <div class="observation">
        <strong>Key insight: Self-correction.</strong> The SAT model initially makes the same error as the base (Step 4: &perp;), but then adds
        <strong>Step 5</strong> as a corrective step. It distinguishes "both must be true" (no) from "at least one must be true" (yes).
        This additional reasoning step &mdash; producing a valid derived conclusion with FOL &mdash; is exactly what the consistency reward encourages.
        <strong>This self-correction pattern is unique to consistency-trained models.</strong>
    </div>
</div>

<!-- Example 5: Llama Disjunctive Syllogism -->
<div class="example">
    <div class="example-header">
        <h3>Example 5: Reconciling Derivation with Question Negation</h3>
        <div>
            <span class="badge badge-model">Llama-3.1 (8B)</span>
            <span class="badge badge-theory">Disjunctive Syllogism</span>
        </div>
    </div>

    <div class="context">
        <div class="context-label">Context (Premises)</div>
        We know that John is not at school or Mary is taking a break from her job. It could be that one or both are true. However, we know that John is indeed at school.
    </div>
    <div class="question">Does this entail that Mary is not taking a break from her job?</div>
    <div class="ground-truth">Ground Truth: No (disjunction + John at school &rarr; Mary IS taking a break)</div>

    <div class="comparison">
        <div class="trace trace-base">
            <div class="trace-header">
                <span class="trace-title">Base Model</span>
                <span class="badge badge-wrong">WRONG &mdash; yes</span>
            </div>
            <div class="trace-content">Step 1: &not;atSchool(John) &or; takingBreak(Mary).
Step 2: atSchool(John).

Step 3: Using Disjunctive Syllogism, since "John is not at school" is false, we can conclude that Mary is taking a break from her job.
(Logic) takingBreak(Mary).

Answer: Yes.</div>
        </div>

        <div class="trace trace-sat">
            <div class="trace-header">
                <span class="trace-title">SAT (RAILS)</span>
                <span class="badge badge-correct">CORRECT &mdash; no</span>
            </div>
            <div class="trace-content">Step 1: &not;atSchool(John) &or; takingBreak(Mary).
Step 2: atSchool(John).

Step 3: Using Disjunctive Syllogism, we conclude that Mary is taking a break from her job.
(Logic) takingBreak(Mary).

<strong style="color:#16a34a;">Step 4: The conclusion is that Mary IS taking a break, so the answer to whether she is NOT taking a break is no.
(Logic) &not;(&not;takingBreak(Mary)).</strong>

Answer: No.</div>
        </div>
    </div>

    <div class="observation">
        <strong>Key insight:</strong> Both models perform the same correct derivation (Mary IS taking a break).
        But the base model answers "Yes" to "Is Mary NOT taking a break?" &mdash; <strong>directly contradicting its own derivation</strong>.
        The SAT model adds <strong>Step 4</strong> to explicitly reconcile the derived conclusion with the question's negation.
        The extra step prevents the base model's contradiction error.
    </div>
</div>

<!-- Structural Patterns -->
<div class="card">
    <h2>Structural Patterns Across Models</h2>
    <p class="section-label">How training conditions affect reasoning structure</p>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>DeepSeek Base</th>
                <th>DeepSeek SAT</th>
                <th>Llama Base</th>
                <th>Llama SAT</th>
                <th>Mistral Base</th>
                <th>Mistral SAT</th>
            </tr>
        </thead>
        <tbody>
            <tr><td style="text-align:left; font-weight:500;">Structured Step N: format</td><td>Rare</td><td>Common</td><td>Always</td><td>Always</td><td>Always</td><td>Always</td></tr>
            <tr><td style="text-align:left; font-weight:500;">FOL annotations</td><td>Rare</td><td>Sometimes</td><td>Always</td><td>Always</td><td>Always</td><td>Always</td></tr>
            <tr><td style="text-align:left; font-weight:500;">Explicit rule naming</td><td>Sometimes</td><td style="color:#16a34a; font-weight:600;">Often</td><td>Rarely</td><td>Rarely</td><td>Sometimes</td><td style="color:#16a34a; font-weight:600;">Often</td></tr>
            <tr><td style="text-align:left; font-weight:500;">Self-correction steps</td><td>Never</td><td style="color:#16a34a; font-weight:600;">Sometimes</td><td>Never</td><td style="color:#16a34a; font-weight:600;">Sometimes</td><td>Never</td><td style="color:#16a34a; font-weight:600;">Sometimes</td></tr>
            <tr><td style="text-align:left; font-weight:500;">Reaches conclusion</td><td>~50%</td><td style="color:#16a34a; font-weight:600;">~90%</td><td>~95%</td><td>~95%</td><td>~85%</td><td style="color:#16a34a; font-weight:600;">~95%</td></tr>
            <tr><td style="text-align:left; font-weight:500;">Avg reasoning steps</td><td>1.0</td><td>2.7</td><td>4.2</td><td>4.1</td><td>3.5</td><td>4.5</td></tr>
        </tbody>
    </table>
</div>

<!-- Summary Findings -->
<div class="card">
    <h2>Summary of Qualitative Findings</h2>

    <div class="finding finding-positive">
        <strong>1. SAT training prevents truncation.</strong>
        The base and answer-only DeepSeek models produce meandering prose that gets cut off at the token limit.
        SAT models produce concise, goal-directed reasoning that terminates with an answer.
    </div>

    <div class="finding finding-positive">
        <strong>2. SAT teaches metacognitive reasoning.</strong>
        SAT-trained models reason about <em>when rules don't apply</em> (Example 2: "there's no valid inference... so the premises don't support the conclusion").
        This higher-order reasoning prevents logical fallacies like denying the antecedent.
    </div>

    <div class="finding finding-positive">
        <strong>3. SAT enables self-correction.</strong>
        In Examples 4 and 5, SAT models initially produce wrong steps, then add corrective steps that reach the right answer.
        This pattern &mdash; wrong derivation followed by correct override &mdash; is unique to consistency-trained models.
    </div>

    <div class="finding finding-insight">
        <strong>4. Answer-only training doesn't change reasoning structure.</strong>
        For DeepSeek, answer-only completions are nearly identical to base &mdash; same rambling prose, same truncation.
        The model may learn to guess answers slightly better but doesn't develop structured reasoning.
    </div>

    <div class="finding finding-caution">
        <strong>5. Diff-Anneal produces structured format but less reliable reasoning.</strong>
        Diff models output well-formatted Step N: (Text)...(Logic)... sequences,
        but sometimes apply rules incorrectly (e.g., Modus Ponens in the wrong direction).
    </div>

    <div class="finding finding-positive">
        <strong>6. The consistency reward's main effect is on proof <em>organization</em>.</strong>
        Models learn to structure premises &rarr; derivation &rarr; conclusion, name inference rules explicitly,
        and reconcile derived conclusions with the question being asked.
    </div>
</div>

<div class="footer">
    RAILS (Reasoning with Automatically Integrated Logical Supervision) &middot; Updated Feb 2026<br>
    <a href="eval_results.html" style="color:#2563eb; text-decoration:none; font-weight:500;">&larr; View Performance Tables</a>
    &middot;
    <a href="index.html" style="color:#2563eb; text-decoration:none; font-weight:500;">Back to Hub</a>
</div>
</body>
</html>
